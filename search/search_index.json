{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"toshi-hazard-store \u00b6 Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only Features \u00b6 Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#toshi-hazard-store","text":"Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only","title":"toshi-hazard-store"},{"location":"#features","text":"Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for toshi-hazard-store. config \u00b6 This module exports comfiguration for the current system. boolean_env ( environ_name , default = 'FALSE' ) \u00b6 Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ]) data_functions \u00b6 weighted_quantile ( values , quantiles , sample_weight = None , values_sorted = False , old_style = False ) \u00b6 Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! :param values: numpy.array with data :param quantiles: array-like with many quantiles needed. Can also be string 'mean' to calculate weighted mean :param sample_weight: array-like of the same length as array :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. :return: numpy.array with computed quantiles. Source code in toshi_hazard_store/data_functions.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def weighted_quantile ( values , quantiles , sample_weight = None , values_sorted = False , old_style = False ): \"\"\"Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! :param values: numpy.array with data :param quantiles: array-like with many quantiles needed. Can also be string 'mean' to calculate weighted mean :param sample_weight: array-like of the same length as `array` :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. :return: numpy.array with computed quantiles. \"\"\" values = np . array ( values ) if sample_weight is None : sample_weight = np . ones ( len ( values )) sample_weight = np . array ( sample_weight ) sample_weight = sample_weight / sum ( sample_weight ) get_mean = False if 'mean' in quantiles : get_mean = True mean_ind = quantiles . index ( 'mean' ) quantiles = quantiles [ 0 : mean_ind ] + quantiles [ mean_ind + 1 :] mean = np . sum ( sample_weight * values ) quantiles = np . array ( [ float ( q ) for q in quantiles ] ) # TODO this section is hacky, need to tighten up API with typing # print(f'QUANTILES: {quantiles}') assert np . all ( quantiles >= 0 ) and np . all ( quantiles <= 1 ), 'quantiles should be in [0, 1]' if not values_sorted : sorter = np . argsort ( values ) values = values [ sorter ] sample_weight = sample_weight [ sorter ] weighted_quantiles = np . cumsum ( sample_weight ) - 0.5 * sample_weight if old_style : # To be convenient with numpy.percentile weighted_quantiles -= weighted_quantiles [ 0 ] weighted_quantiles /= weighted_quantiles [ - 1 ] else : weighted_quantiles /= np . sum ( sample_weight ) wq = np . interp ( quantiles , weighted_quantiles , values ) if get_mean : wq = np . append ( np . append ( wq [ 0 : mean_ind ], np . array ([ mean ])), wq [ mean_ind :]) return wq deaggregate_hazard_mp \u00b6 DisaggHardWorker \u00b6 Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/deaggregate_hazard_mp.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class DisaggHardWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , result_queue ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue self . result_queue = result_queue def run ( self ): print ( f \"worker { self . name } running.\" ) proc_name = self . name while True : nt = self . task_queue . get () if nt is None : # Poison pill means shutdown self . task_queue . task_done () print ( ' %s : Exiting' % proc_name ) break # tic = time.perf_counter() disagg_configs = process_disagg_location_list ( nt . hazard_curves , nt . source_branches , nt . toshi_ids , nt . poes , nt . inv_time , nt . vs30 , nt . locs , nt . aggs , nt . imts , ) self . task_queue . task_done () self . result_queue . put ( disagg_configs ) export_v3 \u00b6 export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ) \u00b6 Extract and same the meta data. Source code in toshi_hazard_store/export_v3.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( oq . reference_vs30_value )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( oq . reference_vs30_value ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj ) locations \u00b6 locations_by_degree ( grid_points , grid_res , point_res ) \u00b6 Produce a dict of key_location: Source code in toshi_hazard_store/locations.py 10 11 12 13 14 15 16 17 18 19 20 def locations_by_degree ( grid_points : List [ Tuple [ float , float ]], grid_res : float , point_res : float ) -> Dict [ str , List [ CodedLocation ]]: \"\"\"Produce a dict of key_location:\"\"\" binned : Dict [ str , CodedLocation ] = dict () for pt in grid_points : bc = CodedLocation ( * pt ) . downsample ( grid_res ) . code if not binned . get ( bc ): binned [ bc ] = [] binned [ bc ] . append ( CodedLocation ( * pt ) . downsample ( point_res )) return binned locations_nz2_chunked ( grid_res = 1.0 , point_res = 0.001 ) \u00b6 used for testing Source code in toshi_hazard_store/locations.py 86 87 88 89 90 91 92 93 94 def locations_nz2_chunked ( grid_res = 1.0 , point_res = 0.001 ): '''used for testing''' chunk_size = 1 # wlg_grid_0_01 = load_grid(\"WLG_0_01_nb_1_1\") cities = [ 'WLG' , 'CHC' , 'KBZ' ] nz34 = [( o [ 'latitude' ], o [ 'longitude' ]) for o in LOCATIONS_BY_ID . values () if o [ 'id' ] in cities ] grid_points = nz34 return locations_by_chunk ( grid_points , point_res , chunk_size ) model \u00b6 drop_tables () \u00b6 Drop em Source code in toshi_hazard_store/model/__init__.py 28 29 30 31 32 def drop_tables (): \"\"\"Drop em\"\"\" drop_tables_v1 () drop_tables_v2 () drop_tables_v3 () migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 21 22 23 24 25 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_v1 () migrate_v2 () migrate_v3 () openquake_v1_model \u00b6 This module defines the pynamodb tables used to store openquake data. LevelValuePairAttribute \u00b6 Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/openquake_v1_model.py 55 56 57 58 59 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False ) ToshiOpenquakeHazardCurveRlzs \u00b6 Bases: Model Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v1_model.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class ToshiOpenquakeHazardCurveRlzs ( Model ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzs- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) imt_loc_rlz_rk = UnicodeAttribute ( range_key = True ) # TODO: check we can actually use this in queries! imt = UnicodeAttribute () loc = UnicodeAttribute () rlz = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) version = VersionAttribute () ToshiOpenquakeHazardCurveStats \u00b6 Bases: Model Stores the individual hazard statistical curves. Source code in toshi_hazard_store/model/openquake_v1_model.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class ToshiOpenquakeHazardCurveStats ( Model ): \"\"\"Stores the individual hazard statistical curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStats- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) imt_loc_agg_rk = UnicodeAttribute ( range_key = True ) imt = UnicodeAttribute () loc = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) version = VersionAttribute () Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v1_model.py 86 87 88 89 90 91 92 93 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStats- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover ToshiOpenquakeHazardMeta \u00b6 Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v1_model.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class ToshiOpenquakeHazardMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) updated = UTCDateTimeAttribute () version = VersionAttribute () # known at configuration haz_sol_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locs = UnicodeSetAttribute () # list of Location codes srcs = UnicodeSetAttribute () # list of source model ids aggs = UnicodeSetAttribute () # list of aggregration/quantile ids e.g. \"0.1. 0.5, mean, 0.9\" inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v1_model.py 25 26 27 28 29 30 31 32 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v1_model.py 118 119 120 121 122 123 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v1_model.py 109 110 111 112 113 114 115 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) openquake_v2_model \u00b6 This module defines the pynamodb tables used to store openquake v2 data. IMTValuesAttribute \u00b6 Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/openquake_v2_model.py 19 20 21 22 23 24 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute ) ToshiOpenquakeHazardCurveRlzsV2 \u00b6 Bases: Model Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v2_model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ToshiOpenquakeHazardCurveRlzsV2 ( Model ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) loc_rlz_rk = UnicodeAttribute ( range_key = True ) # TODO: check we can actually use this in queries! loc = UnicodeAttribute () rlz = UnicodeAttribute () lat = FloatAttribute () lon = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) values = ListAttribute ( of = IMTValuesAttribute ) version = VersionAttribute () Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v2_model.py 30 31 32 33 34 35 36 37 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover ToshiOpenquakeHazardCurveStatsV2 \u00b6 Bases: Model Stores the individual hazard statistical curves. Source code in toshi_hazard_store/model/openquake_v2_model.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class ToshiOpenquakeHazardCurveStatsV2 ( Model ): \"\"\"Stores the individual hazard statistical curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStatsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) loc_agg_rk = UnicodeAttribute ( range_key = True ) loc = UnicodeAttribute () agg = UnicodeAttribute () lat = FloatAttribute () lon = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) values = ListAttribute ( of = IMTValuesAttribute ) version = VersionAttribute () Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v2_model.py 55 56 57 58 59 60 61 62 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStatsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v2_model.py 92 93 94 95 96 97 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v2_model.py 83 84 85 86 87 88 89 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) openquake_v3_model \u00b6 This module defines the pynamodb tables used to store openquake data. Third iteration HazardAggregation \u00b6 Bases: LocationIndexedModel Stores aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class HazardAggregation ( LocationIndexedModel ): \"\"\"Stores aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) # aggregation_info = # details about the aggregation # count of aggregated items # aggregation configuration: filtering, grouping # subject: rlzs or aggregations # requested # time_seconds: # started: # Secondary Index attributes # index1 = vs30_nloc1_gt_rlz_index() # index1_rk = UnicodeAttribute() def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 138 139 140 141 142 143 144 145 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 165 166 167 168 169 170 171 172 173 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self LocationIndexedModel \u00b6 Bases: Model Model base class. Source code in toshi_hazard_store/model/openquake_v3_model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self OpenquakeRealization \u00b6 Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 179 180 181 182 183 184 185 186 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 199 200 201 202 203 204 205 206 207 208 209 210 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self ToshiOpenquakeMeta \u00b6 Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v3_model.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 35 36 37 38 39 40 41 42 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover vs30_nloc001_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 87 88 89 90 91 92 93 94 95 96 97 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True ) vs30_nloc1_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 74 75 76 77 78 79 80 81 82 83 84 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True ) drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v3_model.py 225 226 227 228 229 230 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v3_model.py 216 217 218 219 220 221 222 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) multi_batch \u00b6 DynamoBatchWorker \u00b6 Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") if self . model == model . ToshiOpenquakeHazardCurveStatsV2 : query . batch_save_hcurve_stats_v2 ( self . toshi_id , models = models ) elif self . model == model . ToshiOpenquakeHazardCurveRlzsV2 : query . batch_save_hcurve_rlzs_v2 ( self . toshi_id , models = models ) elif self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" ) pynamodb_settings \u00b6 pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered. query \u00b6 Queries for saving and retrieving openquake hazard results with convenience. batch_save_hcurve_rlzs ( toshi_id , models ) \u00b6 Save list of ToshiOpenquakeHazardCurveRlzs updating hash and range keys. Source code in toshi_hazard_store/query.py 16 17 18 19 20 21 22 def batch_save_hcurve_rlzs ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveRlzs ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzs updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveRlzs . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . imt_loc_rlz_rk = f \" { item . imt } : { item . loc } : { item . rlz } \" batch . save ( item ) batch_save_hcurve_rlzs_v2 ( toshi_id , models ) \u00b6 Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys. Source code in toshi_hazard_store/query.py 25 26 27 28 29 30 31 def batch_save_hcurve_rlzs_v2 ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveRlzsV2 ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveRlzsV2 . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . loc_rlz_rk = f \" { item . loc } : { item . rlz } \" batch . save ( item ) batch_save_hcurve_stats ( toshi_id , models ) \u00b6 Save list of ToshiOpenquakeHazardCurveStats updating hash and range keys. Source code in toshi_hazard_store/query.py 7 8 9 10 11 12 13 def batch_save_hcurve_stats ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveStats ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveStats updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveStats . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . imt_loc_agg_rk = f \" { item . imt } : { item . loc } : { item . agg } \" batch . save ( item ) batch_save_hcurve_stats_v2 ( toshi_id , models ) \u00b6 Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys. Source code in toshi_hazard_store/query.py 34 35 36 37 38 39 40 def batch_save_hcurve_stats_v2 ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveStatsV2 ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveStatsV2 . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . loc_agg_rk = f \" { item . loc } : { item . agg } \" batch . save ( item ) get_hazard_metadata ( haz_sol_ids = None , vs30_vals = None ) \u00b6 Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def get_hazard_metadata ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOHM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOHM . haz_sol_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOHM . vs30 . is_in ( * vs30_vals ) for hit in model . ToshiOpenquakeHazardMeta . query ( \"ToshiOpenquakeHazardMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit ) get_hazard_rlz_curves ( haz_sol_id , imts = None , locs = None , rlzs = None ) \u00b6 Use ToshiOpenquakeHazardCurveRlzs.imt_loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_hazard_rlz_curves ( haz_sol_id : str , imts : Iterable [ str ] = None , locs : Iterable [ str ] = None , rlzs : Iterable [ str ] = None , ) -> Iterator [ mOHCR ]: \"\"\"Use ToshiOpenquakeHazardCurveRlzs.imt_loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if imts : first_imt = sorted ( imts )[ 0 ] range_key_first_val += f \" { first_imt } \" condition_expr = condition_expr & mOHCR . imt . is_in ( * imts ) if locs : condition_expr = condition_expr & mOHCR . loc . is_in ( * locs ) if rlzs : condition_expr = condition_expr & mOHCR . rlz . is_in ( * rlzs ) if imts and locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \": { first_loc } \" if imts and locs and rlzs : first_rlz = sorted ( rlzs )[ 0 ] range_key_first_val += f \": { first_rlz } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCR . query ( haz_sol_id , mOHCR . imt_loc_rlz_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCR . query ( haz_sol_id , mOHCR . imt_loc_rlz_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves: qry { qry } \" ) for hit in qry : yield ( hit ) get_hazard_rlz_curves_v2 ( haz_sol_id , imts = [], locs = [], rlzs = []) \u00b6 Use mOHCR2.loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def get_hazard_rlz_curves_v2 ( haz_sol_id : str , imts : Iterable [ str ] = [], locs : Iterable [ str ] = [], rlzs : Iterable [ str ] = [], ) -> Iterator [ mOHCR2 ]: \"\"\"Use mOHCR2.loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None # if imts: # first_imt = sorted(imts)[0] # range_key_first_val += f\"{first_imt}\" # condition_expr = condition_expr & mOHCR.imt.is_in(*imts) if locs : condition_expr = condition_expr & mOHCR2 . loc . is_in ( * locs ) if rlzs : condition_expr = condition_expr & mOHCR2 . rlz . is_in ( * rlzs ) if locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \" { first_loc } \" if locs and rlzs : first_rlz = sorted ( rlzs )[ 0 ] range_key_first_val += f \": { first_rlz } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCR2 . query ( haz_sol_id , mOHCR2 . loc_rlz_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCR2 . query ( haz_sol_id , mOHCR2 . loc_rlz_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves_v2: qry { qry } \" ) for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit ) get_hazard_stats_curves ( haz_sol_id , imts = None , locs = None , aggs = None ) \u00b6 Use ToshiOpenquakeHazardCurveStats.imt_loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_hazard_stats_curves ( haz_sol_id : str , imts : Iterable [ str ] = None , locs : Iterable [ str ] = None , aggs : Iterable [ str ] = None , ) -> Iterator [ mOHCS ]: \"\"\"Use ToshiOpenquakeHazardCurveStats.imt_loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if imts : first_imt = sorted ( imts )[ 0 ] range_key_first_val += f \" { first_imt } \" condition_expr = condition_expr & mOHCS . imt . is_in ( * imts ) if locs : condition_expr = condition_expr & mOHCS . loc . is_in ( * locs ) if aggs : condition_expr = condition_expr & mOHCS . agg . is_in ( * aggs ) if imts and locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \": { first_loc } \" if imts and locs and aggs : first_agg = sorted ( aggs )[ 0 ] range_key_first_val += f \": { first_agg } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCS . query ( haz_sol_id , mOHCS . imt_loc_agg_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCS . query ( haz_sol_id , mOHCS . imt_loc_agg_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_stats_curves: qry { qry } \" ) for hit in qry : yield ( hit ) get_hazard_stats_curves_v2 ( haz_sol_id , imts = [], locs = [], aggs = []) \u00b6 Use mOHCS2.loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def get_hazard_stats_curves_v2 ( haz_sol_id : str , imts : Iterable [ str ] = [], locs : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mOHCS2 ]: \"\"\"Use mOHCS2.loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if locs : condition_expr = condition_expr & mOHCS2 . loc . is_in ( * locs ) if aggs : condition_expr = condition_expr & mOHCS2 . agg . is_in ( * aggs ) if locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \" { first_loc } \" if locs and aggs : first_agg = sorted ( aggs )[ 0 ] range_key_first_val += f \": { first_agg } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCS2 . query ( haz_sol_id , mOHCS2 . loc_agg_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCS2 . query ( haz_sol_id , mOHCS2 . loc_agg_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_stats_curves_v2: qry { qry } \" ) for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit ) query_v3 \u00b6 Queries for saving and retrieving openquake hazard results with convenience. get_hazard_curves ( locs = [], vs30s = [], hazard_model_ids = [], imts = [], aggs = []) \u00b6 Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query_v3.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : first_vs30 = sorted ( vs30s )[ 0 ] sort_key_first_val += f \": { first_vs30 } \" if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): print ( f 'hash_key { hash_location_code } ' ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) print ( f 'sort_key_first_val { sort_key_first_val } ' ) print ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves_v3: qry { qry } \" ) for hit in qry : yield ( hit ) get_hazard_metadata_v3 ( haz_sol_ids = None , vs30_vals = None ) \u00b6 Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query_v3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit ) get_rlz_curves_v3 ( locs = [], vs30s = [], rlzs = [], tids = [], imts = []) \u00b6 Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query_v3.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] # print() # print(f'res {res} locs {locs}') condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : first_vs30 = sorted ( vs30s )[ 0 ] sort_key_first_val += f \": { first_vs30 } \" if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit ) transform \u00b6 Helper functions to export an openquake calculation and save it with toshi-hazard-store. export_meta ( toshi_id , dstore , * , force_normalized_sites = False ) \u00b6 Extract and same the meta data. Source code in toshi_hazard_store/transform.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def export_meta ( toshi_id , dstore , * , force_normalized_sites : bool = False ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] sitemesh = get_sites ( dstore [ 'sitecol' ]) source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) quantiles = [ str ( q ) for q in vars ( oq ) . get ( 'quantiles' , [])] + [ 'mean' ] # mean is default, other values come from the config df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeHazardMeta ( partition_key = \"ToshiOpenquakeHazardMeta\" , updated = dt . datetime . now ( tzutc ()), vs30 = oq . reference_vs30_value , # vs30 value haz_sol_id = toshi_id , imts = list ( oq . imtls . keys ()), # list of IMTs locs = [ normalise_site_code ( loc , force_normalized_sites ) . code for loc in sitemesh . tolist () ], # list of Location codes, can be normalised # important configuration arguments aggs = quantiles , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . hazsol_vs30_rk = f \" { obj . haz_sol_id } : { obj . vs30 } \" obj . save () parse_logic_tree_branches ( file_id ) \u00b6 Extract the dataframes. Source code in toshi_hazard_store/transform.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt utils \u00b6 Common utilities. normalise_site_code ( oq_site_object , force_normalized = False ) \u00b6 Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"Modules"},{"location":"api/#toshi_hazard_store.config","text":"This module exports comfiguration for the current system.","title":"config"},{"location":"api/#toshi_hazard_store.config.boolean_env","text":"Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ])","title":"boolean_env()"},{"location":"api/#toshi_hazard_store.data_functions","text":"","title":"data_functions"},{"location":"api/#toshi_hazard_store.data_functions.weighted_quantile","text":"Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! :param values: numpy.array with data :param quantiles: array-like with many quantiles needed. Can also be string 'mean' to calculate weighted mean :param sample_weight: array-like of the same length as array :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. :return: numpy.array with computed quantiles. Source code in toshi_hazard_store/data_functions.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def weighted_quantile ( values , quantiles , sample_weight = None , values_sorted = False , old_style = False ): \"\"\"Very close to numpy.percentile, but supports weights. NOTE: quantiles should be in [0, 1]! :param values: numpy.array with data :param quantiles: array-like with many quantiles needed. Can also be string 'mean' to calculate weighted mean :param sample_weight: array-like of the same length as `array` :param values_sorted: bool, if True, then will avoid sorting of initial array :param old_style: if True, will correct output to be consistent with numpy.percentile. :return: numpy.array with computed quantiles. \"\"\" values = np . array ( values ) if sample_weight is None : sample_weight = np . ones ( len ( values )) sample_weight = np . array ( sample_weight ) sample_weight = sample_weight / sum ( sample_weight ) get_mean = False if 'mean' in quantiles : get_mean = True mean_ind = quantiles . index ( 'mean' ) quantiles = quantiles [ 0 : mean_ind ] + quantiles [ mean_ind + 1 :] mean = np . sum ( sample_weight * values ) quantiles = np . array ( [ float ( q ) for q in quantiles ] ) # TODO this section is hacky, need to tighten up API with typing # print(f'QUANTILES: {quantiles}') assert np . all ( quantiles >= 0 ) and np . all ( quantiles <= 1 ), 'quantiles should be in [0, 1]' if not values_sorted : sorter = np . argsort ( values ) values = values [ sorter ] sample_weight = sample_weight [ sorter ] weighted_quantiles = np . cumsum ( sample_weight ) - 0.5 * sample_weight if old_style : # To be convenient with numpy.percentile weighted_quantiles -= weighted_quantiles [ 0 ] weighted_quantiles /= weighted_quantiles [ - 1 ] else : weighted_quantiles /= np . sum ( sample_weight ) wq = np . interp ( quantiles , weighted_quantiles , values ) if get_mean : wq = np . append ( np . append ( wq [ 0 : mean_ind ], np . array ([ mean ])), wq [ mean_ind :]) return wq","title":"weighted_quantile()"},{"location":"api/#toshi_hazard_store.deaggregate_hazard_mp","text":"","title":"deaggregate_hazard_mp"},{"location":"api/#toshi_hazard_store.deaggregate_hazard_mp.DisaggHardWorker","text":"Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/deaggregate_hazard_mp.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class DisaggHardWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , result_queue ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue self . result_queue = result_queue def run ( self ): print ( f \"worker { self . name } running.\" ) proc_name = self . name while True : nt = self . task_queue . get () if nt is None : # Poison pill means shutdown self . task_queue . task_done () print ( ' %s : Exiting' % proc_name ) break # tic = time.perf_counter() disagg_configs = process_disagg_location_list ( nt . hazard_curves , nt . source_branches , nt . toshi_ids , nt . poes , nt . inv_time , nt . vs30 , nt . locs , nt . aggs , nt . imts , ) self . task_queue . task_done () self . result_queue . put ( disagg_configs )","title":"DisaggHardWorker"},{"location":"api/#toshi_hazard_store.export_v3","text":"","title":"export_v3"},{"location":"api/#toshi_hazard_store.export_v3.export_meta_v3","text":"Extract and same the meta data. Source code in toshi_hazard_store/export_v3.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( oq . reference_vs30_value )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( oq . reference_vs30_value ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj )","title":"export_meta_v3()"},{"location":"api/#toshi_hazard_store.locations","text":"","title":"locations"},{"location":"api/#toshi_hazard_store.locations.locations_by_degree","text":"Produce a dict of key_location: Source code in toshi_hazard_store/locations.py 10 11 12 13 14 15 16 17 18 19 20 def locations_by_degree ( grid_points : List [ Tuple [ float , float ]], grid_res : float , point_res : float ) -> Dict [ str , List [ CodedLocation ]]: \"\"\"Produce a dict of key_location:\"\"\" binned : Dict [ str , CodedLocation ] = dict () for pt in grid_points : bc = CodedLocation ( * pt ) . downsample ( grid_res ) . code if not binned . get ( bc ): binned [ bc ] = [] binned [ bc ] . append ( CodedLocation ( * pt ) . downsample ( point_res )) return binned","title":"locations_by_degree()"},{"location":"api/#toshi_hazard_store.locations.locations_nz2_chunked","text":"used for testing Source code in toshi_hazard_store/locations.py 86 87 88 89 90 91 92 93 94 def locations_nz2_chunked ( grid_res = 1.0 , point_res = 0.001 ): '''used for testing''' chunk_size = 1 # wlg_grid_0_01 = load_grid(\"WLG_0_01_nb_1_1\") cities = [ 'WLG' , 'CHC' , 'KBZ' ] nz34 = [( o [ 'latitude' ], o [ 'longitude' ]) for o in LOCATIONS_BY_ID . values () if o [ 'id' ] in cities ] grid_points = nz34 return locations_by_chunk ( grid_points , point_res , chunk_size )","title":"locations_nz2_chunked()"},{"location":"api/#toshi_hazard_store.model","text":"","title":"model"},{"location":"api/#toshi_hazard_store.model.drop_tables","text":"Drop em Source code in toshi_hazard_store/model/__init__.py 28 29 30 31 32 def drop_tables (): \"\"\"Drop em\"\"\" drop_tables_v1 () drop_tables_v2 () drop_tables_v3 ()","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 21 22 23 24 25 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_v1 () migrate_v2 () migrate_v3 ()","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model","text":"This module defines the pynamodb tables used to store openquake data.","title":"openquake_v1_model"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.LevelValuePairAttribute","text":"Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/openquake_v1_model.py 55 56 57 58 59 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False )","title":"LevelValuePairAttribute"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.ToshiOpenquakeHazardCurveRlzs","text":"Bases: Model Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v1_model.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class ToshiOpenquakeHazardCurveRlzs ( Model ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzs- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) imt_loc_rlz_rk = UnicodeAttribute ( range_key = True ) # TODO: check we can actually use this in queries! imt = UnicodeAttribute () loc = UnicodeAttribute () rlz = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) version = VersionAttribute ()","title":"ToshiOpenquakeHazardCurveRlzs"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.ToshiOpenquakeHazardCurveStats","text":"Bases: Model Stores the individual hazard statistical curves. Source code in toshi_hazard_store/model/openquake_v1_model.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class ToshiOpenquakeHazardCurveStats ( Model ): \"\"\"Stores the individual hazard statistical curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStats- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) imt_loc_agg_rk = UnicodeAttribute ( range_key = True ) imt = UnicodeAttribute () loc = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) version = VersionAttribute ()","title":"ToshiOpenquakeHazardCurveStats"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.ToshiOpenquakeHazardCurveStats.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v1_model.py 86 87 88 89 90 91 92 93 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStats- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.ToshiOpenquakeHazardMeta","text":"Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v1_model.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class ToshiOpenquakeHazardMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) updated = UTCDateTimeAttribute () version = VersionAttribute () # known at configuration haz_sol_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locs = UnicodeSetAttribute () # list of Location codes srcs = UnicodeSetAttribute () # list of source model ids aggs = UnicodeSetAttribute () # list of aggregration/quantile ids e.g. \"0.1. 0.5, mean, 0.9\" inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON","title":"ToshiOpenquakeHazardMeta"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.ToshiOpenquakeHazardMeta.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v1_model.py 25 26 27 28 29 30 31 32 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v1_model.py 118 119 120 121 122 123 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v1_model.py 109 110 111 112 113 114 115 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model","text":"This module defines the pynamodb tables used to store openquake v2 data.","title":"openquake_v2_model"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.IMTValuesAttribute","text":"Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/openquake_v2_model.py 19 20 21 22 23 24 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute )","title":"IMTValuesAttribute"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.ToshiOpenquakeHazardCurveRlzsV2","text":"Bases: Model Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v2_model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class ToshiOpenquakeHazardCurveRlzsV2 ( Model ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) loc_rlz_rk = UnicodeAttribute ( range_key = True ) # TODO: check we can actually use this in queries! loc = UnicodeAttribute () rlz = UnicodeAttribute () lat = FloatAttribute () lon = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) values = ListAttribute ( of = IMTValuesAttribute ) version = VersionAttribute ()","title":"ToshiOpenquakeHazardCurveRlzsV2"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.ToshiOpenquakeHazardCurveRlzsV2.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v2_model.py 30 31 32 33 34 35 36 37 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveRlzsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.ToshiOpenquakeHazardCurveStatsV2","text":"Bases: Model Stores the individual hazard statistical curves. Source code in toshi_hazard_store/model/openquake_v2_model.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class ToshiOpenquakeHazardCurveStatsV2 ( Model ): \"\"\"Stores the individual hazard statistical curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStatsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover haz_sol_id = UnicodeAttribute ( hash_key = True ) loc_agg_rk = UnicodeAttribute ( range_key = True ) loc = UnicodeAttribute () agg = UnicodeAttribute () lat = FloatAttribute () lon = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) values = ListAttribute ( of = IMTValuesAttribute ) version = VersionAttribute ()","title":"ToshiOpenquakeHazardCurveStatsV2"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.ToshiOpenquakeHazardCurveStatsV2.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v2_model.py 55 56 57 58 59 60 61 62 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"ToshiOpenquakeHazardCurveStatsV2- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v2_model.py 92 93 94 95 96 97 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v2_model.py 83 84 85 86 87 88 89 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model","text":"This module defines the pynamodb tables used to store openquake data. Third iteration","title":"openquake_v3_model"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation","text":"Bases: LocationIndexedModel Stores aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class HazardAggregation ( LocationIndexedModel ): \"\"\"Stores aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) # aggregation_info = # details about the aggregation # count of aggregated items # aggregation configuration: filtering, grouping # subject: rlzs or aggregations # requested # time_seconds: # started: # Secondary Index attributes # index1 = vs30_nloc1_gt_rlz_index() # index1_rk = UnicodeAttribute() def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self","title":"HazardAggregation"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 138 139 140 141 142 143 144 145 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 165 166 167 168 169 170 171 172 173 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.LocationIndexedModel","text":"Bases: Model Model base class. Source code in toshi_hazard_store/model/openquake_v3_model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"LocationIndexedModel"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.LocationIndexedModel.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization","text":"Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"OpenquakeRealization"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 179 180 181 182 183 184 185 186 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 199 200 201 202 203 204 205 206 207 208 209 210 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( 3 ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.ToshiOpenquakeMeta","text":"Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v3_model.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON","title":"ToshiOpenquakeMeta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.ToshiOpenquakeMeta.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 35 36 37 38 39 40 41 42 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.vs30_nloc001_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 87 88 89 90 91 92 93 94 95 96 97 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc001_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.vs30_nloc1_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 74 75 76 77 78 79 80 81 82 83 84 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc1_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v3_model.py 225 226 227 228 229 230 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v3_model.py 216 217 218 219 220 221 222 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.multi_batch","text":"","title":"multi_batch"},{"location":"api/#toshi_hazard_store.multi_batch.DynamoBatchWorker","text":"Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") if self . model == model . ToshiOpenquakeHazardCurveStatsV2 : query . batch_save_hcurve_stats_v2 ( self . toshi_id , models = models ) elif self . model == model . ToshiOpenquakeHazardCurveRlzsV2 : query . batch_save_hcurve_rlzs_v2 ( self . toshi_id , models = models ) elif self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" )","title":"DynamoBatchWorker"},{"location":"api/#toshi_hazard_store.pynamodb_settings","text":"pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered.","title":"pynamodb_settings"},{"location":"api/#toshi_hazard_store.query","text":"Queries for saving and retrieving openquake hazard results with convenience.","title":"query"},{"location":"api/#toshi_hazard_store.query.batch_save_hcurve_rlzs","text":"Save list of ToshiOpenquakeHazardCurveRlzs updating hash and range keys. Source code in toshi_hazard_store/query.py 16 17 18 19 20 21 22 def batch_save_hcurve_rlzs ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveRlzs ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzs updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveRlzs . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . imt_loc_rlz_rk = f \" { item . imt } : { item . loc } : { item . rlz } \" batch . save ( item )","title":"batch_save_hcurve_rlzs()"},{"location":"api/#toshi_hazard_store.query.batch_save_hcurve_rlzs_v2","text":"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys. Source code in toshi_hazard_store/query.py 25 26 27 28 29 30 31 def batch_save_hcurve_rlzs_v2 ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveRlzsV2 ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveRlzsV2 . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . loc_rlz_rk = f \" { item . loc } : { item . rlz } \" batch . save ( item )","title":"batch_save_hcurve_rlzs_v2()"},{"location":"api/#toshi_hazard_store.query.batch_save_hcurve_stats","text":"Save list of ToshiOpenquakeHazardCurveStats updating hash and range keys. Source code in toshi_hazard_store/query.py 7 8 9 10 11 12 13 def batch_save_hcurve_stats ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveStats ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveStats updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveStats . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . imt_loc_agg_rk = f \" { item . imt } : { item . loc } : { item . agg } \" batch . save ( item )","title":"batch_save_hcurve_stats()"},{"location":"api/#toshi_hazard_store.query.batch_save_hcurve_stats_v2","text":"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys. Source code in toshi_hazard_store/query.py 34 35 36 37 38 39 40 def batch_save_hcurve_stats_v2 ( toshi_id : str , models : Iterable [ model . ToshiOpenquakeHazardCurveStatsV2 ]) -> None : \"\"\"Save list of ToshiOpenquakeHazardCurveRlzsV2 updating hash and range keys.\"\"\" with model . ToshiOpenquakeHazardCurveStatsV2 . batch_write () as batch : for item in models : item . haz_sol_id = toshi_id item . loc_agg_rk = f \" { item . loc } : { item . agg } \" batch . save ( item )","title":"batch_save_hcurve_stats_v2()"},{"location":"api/#toshi_hazard_store.query.get_hazard_metadata","text":"Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def get_hazard_metadata ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOHM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOHM . haz_sol_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOHM . vs30 . is_in ( * vs30_vals ) for hit in model . ToshiOpenquakeHazardMeta . query ( \"ToshiOpenquakeHazardMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit )","title":"get_hazard_metadata()"},{"location":"api/#toshi_hazard_store.query.get_hazard_rlz_curves","text":"Use ToshiOpenquakeHazardCurveRlzs.imt_loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_hazard_rlz_curves ( haz_sol_id : str , imts : Iterable [ str ] = None , locs : Iterable [ str ] = None , rlzs : Iterable [ str ] = None , ) -> Iterator [ mOHCR ]: \"\"\"Use ToshiOpenquakeHazardCurveRlzs.imt_loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if imts : first_imt = sorted ( imts )[ 0 ] range_key_first_val += f \" { first_imt } \" condition_expr = condition_expr & mOHCR . imt . is_in ( * imts ) if locs : condition_expr = condition_expr & mOHCR . loc . is_in ( * locs ) if rlzs : condition_expr = condition_expr & mOHCR . rlz . is_in ( * rlzs ) if imts and locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \": { first_loc } \" if imts and locs and rlzs : first_rlz = sorted ( rlzs )[ 0 ] range_key_first_val += f \": { first_rlz } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCR . query ( haz_sol_id , mOHCR . imt_loc_rlz_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCR . query ( haz_sol_id , mOHCR . imt_loc_rlz_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_hazard_rlz_curves()"},{"location":"api/#toshi_hazard_store.query.get_hazard_rlz_curves_v2","text":"Use mOHCR2.loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def get_hazard_rlz_curves_v2 ( haz_sol_id : str , imts : Iterable [ str ] = [], locs : Iterable [ str ] = [], rlzs : Iterable [ str ] = [], ) -> Iterator [ mOHCR2 ]: \"\"\"Use mOHCR2.loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None # if imts: # first_imt = sorted(imts)[0] # range_key_first_val += f\"{first_imt}\" # condition_expr = condition_expr & mOHCR.imt.is_in(*imts) if locs : condition_expr = condition_expr & mOHCR2 . loc . is_in ( * locs ) if rlzs : condition_expr = condition_expr & mOHCR2 . rlz . is_in ( * rlzs ) if locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \" { first_loc } \" if locs and rlzs : first_rlz = sorted ( rlzs )[ 0 ] range_key_first_val += f \": { first_rlz } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCR2 . query ( haz_sol_id , mOHCR2 . loc_rlz_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCR2 . query ( haz_sol_id , mOHCR2 . loc_rlz_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves_v2: qry { qry } \" ) for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit )","title":"get_hazard_rlz_curves_v2()"},{"location":"api/#toshi_hazard_store.query.get_hazard_stats_curves","text":"Use ToshiOpenquakeHazardCurveStats.imt_loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_hazard_stats_curves ( haz_sol_id : str , imts : Iterable [ str ] = None , locs : Iterable [ str ] = None , aggs : Iterable [ str ] = None , ) -> Iterator [ mOHCS ]: \"\"\"Use ToshiOpenquakeHazardCurveStats.imt_loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if imts : first_imt = sorted ( imts )[ 0 ] range_key_first_val += f \" { first_imt } \" condition_expr = condition_expr & mOHCS . imt . is_in ( * imts ) if locs : condition_expr = condition_expr & mOHCS . loc . is_in ( * locs ) if aggs : condition_expr = condition_expr & mOHCS . agg . is_in ( * aggs ) if imts and locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \": { first_loc } \" if imts and locs and aggs : first_agg = sorted ( aggs )[ 0 ] range_key_first_val += f \": { first_agg } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCS . query ( haz_sol_id , mOHCS . imt_loc_agg_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCS . query ( haz_sol_id , mOHCS . imt_loc_agg_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_stats_curves: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_hazard_stats_curves()"},{"location":"api/#toshi_hazard_store.query.get_hazard_stats_curves_v2","text":"Use mOHCS2.loc_agg_rk range key as much as possible. Source code in toshi_hazard_store/query.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def get_hazard_stats_curves_v2 ( haz_sol_id : str , imts : Iterable [ str ] = [], locs : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mOHCS2 ]: \"\"\"Use mOHCS2.loc_agg_rk range key as much as possible.\"\"\" range_key_first_val = \"\" condition_expr = None if locs : condition_expr = condition_expr & mOHCS2 . loc . is_in ( * locs ) if aggs : condition_expr = condition_expr & mOHCS2 . agg . is_in ( * aggs ) if locs : first_loc = sorted ( locs )[ 0 ] range_key_first_val += f \" { first_loc } \" if locs and aggs : first_agg = sorted ( aggs )[ 0 ] range_key_first_val += f \": { first_agg } \" print ( f \"range_key_first_val: { range_key_first_val } \" ) print ( condition_expr ) if range_key_first_val : qry = mOHCS2 . query ( haz_sol_id , mOHCS2 . loc_agg_rk >= range_key_first_val , filter_condition = condition_expr ) else : qry = mOHCS2 . query ( haz_sol_id , mOHCS2 . loc_agg_rk >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_stats_curves_v2: qry { qry } \" ) for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit )","title":"get_hazard_stats_curves_v2()"},{"location":"api/#toshi_hazard_store.query_v3","text":"Queries for saving and retrieving openquake hazard results with convenience.","title":"query_v3"},{"location":"api/#toshi_hazard_store.query_v3.get_hazard_curves","text":"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query_v3.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : first_vs30 = sorted ( vs30s )[ 0 ] sort_key_first_val += f \": { first_vs30 } \" if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): print ( f 'hash_key { hash_location_code } ' ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) print ( f 'sort_key_first_val { sort_key_first_val } ' ) print ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) print ( f \"get_hazard_rlz_curves_v3: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_hazard_curves()"},{"location":"api/#toshi_hazard_store.query_v3.get_hazard_metadata_v3","text":"Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query_v3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit )","title":"get_hazard_metadata_v3()"},{"location":"api/#toshi_hazard_store.query_v3.get_rlz_curves_v3","text":"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query_v3.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] # print() # print(f'res {res} locs {locs}') condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : first_vs30 = sorted ( vs30s )[ 0 ] sort_key_first_val += f \": { first_vs30 } \" if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit )","title":"get_rlz_curves_v3()"},{"location":"api/#toshi_hazard_store.transform","text":"Helper functions to export an openquake calculation and save it with toshi-hazard-store.","title":"transform"},{"location":"api/#toshi_hazard_store.transform.export_meta","text":"Extract and same the meta data. Source code in toshi_hazard_store/transform.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def export_meta ( toshi_id , dstore , * , force_normalized_sites : bool = False ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] sitemesh = get_sites ( dstore [ 'sitecol' ]) source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) quantiles = [ str ( q ) for q in vars ( oq ) . get ( 'quantiles' , [])] + [ 'mean' ] # mean is default, other values come from the config df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeHazardMeta ( partition_key = \"ToshiOpenquakeHazardMeta\" , updated = dt . datetime . now ( tzutc ()), vs30 = oq . reference_vs30_value , # vs30 value haz_sol_id = toshi_id , imts = list ( oq . imtls . keys ()), # list of IMTs locs = [ normalise_site_code ( loc , force_normalized_sites ) . code for loc in sitemesh . tolist () ], # list of Location codes, can be normalised # important configuration arguments aggs = quantiles , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . hazsol_vs30_rk = f \" { obj . haz_sol_id } : { obj . vs30 } \" obj . save ()","title":"export_meta()"},{"location":"api/#toshi_hazard_store.transform.parse_logic_tree_branches","text":"Extract the dataframes. Source code in toshi_hazard_store/transform.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt","title":"parse_logic_tree_branches()"},{"location":"api/#toshi_hazard_store.utils","text":"Common utilities.","title":"utils"},{"location":"api/#toshi_hazard_store.utils.normalise_site_code","text":"Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"normalise_site_code()"},{"location":"changelog/","text":"Changelog \u00b6 [0.5.0] - 2022-08-03 \u00b6 Added \u00b6 V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling. Removed \u00b6 realisation aggregration computations. These have moving to toshi-hazard-post [0.4.1] - 2022-06-22 \u00b6 Added \u00b6 multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types Changed \u00b6 V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules. [0.4.0] - 2022-06-10 \u00b6 Added \u00b6 new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency. Changed \u00b6 meta dataframes are cut back to dstore defaults to minimise size. [0.3.2] - 2022-05-30 \u00b6 Added \u00b6 meta.aggs attribute meta.inv_tme attribute Changed \u00b6 store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks). [0.3.1] - 2022-05-29 \u00b6 Changed \u00b6 updated usage. [0.3.0] - 2022-05-28 \u00b6 Added \u00b6 store_hazard script for openquake systems. Changed \u00b6 tightened up model attributes names. [0.2.0] - 2022-05-27 \u00b6 Added \u00b6 query api improvements added meta table new query methods for meta and rlzs Changed \u00b6 moved vs30 from curves to meta updated docs [0.1.3] - 2022-05-26 \u00b6 Changed \u00b6 fixed mkdoc rendering of python & markdown. [0.1.2] - 2022-05-26 \u00b6 Changed \u00b6 fix poetry lockfile [0.1.1] - 2022-05-26 \u00b6 Added \u00b6 First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#050---2022-08-03","text":"","title":"[0.5.0] - 2022-08-03"},{"location":"changelog/#added","text":"V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling.","title":"Added"},{"location":"changelog/#removed","text":"realisation aggregration computations. These have moving to toshi-hazard-post","title":"Removed"},{"location":"changelog/#041---2022-06-22","text":"","title":"[0.4.1] - 2022-06-22"},{"location":"changelog/#added_1","text":"multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types","title":"Added"},{"location":"changelog/#changed","text":"V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules.","title":"Changed"},{"location":"changelog/#040---2022-06-10","text":"","title":"[0.4.0] - 2022-06-10"},{"location":"changelog/#added_2","text":"new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency.","title":"Added"},{"location":"changelog/#changed_1","text":"meta dataframes are cut back to dstore defaults to minimise size.","title":"Changed"},{"location":"changelog/#032---2022-05-30","text":"","title":"[0.3.2] - 2022-05-30"},{"location":"changelog/#added_3","text":"meta.aggs attribute meta.inv_tme attribute","title":"Added"},{"location":"changelog/#changed_2","text":"store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks).","title":"Changed"},{"location":"changelog/#031---2022-05-29","text":"","title":"[0.3.1] - 2022-05-29"},{"location":"changelog/#changed_3","text":"updated usage.","title":"Changed"},{"location":"changelog/#030---2022-05-28","text":"","title":"[0.3.0] - 2022-05-28"},{"location":"changelog/#added_4","text":"store_hazard script for openquake systems.","title":"Added"},{"location":"changelog/#changed_4","text":"tightened up model attributes names.","title":"Changed"},{"location":"changelog/#020---2022-05-27","text":"","title":"[0.2.0] - 2022-05-27"},{"location":"changelog/#added_5","text":"query api improvements added meta table new query methods for meta and rlzs","title":"Added"},{"location":"changelog/#changed_5","text":"moved vs30 from curves to meta updated docs","title":"Changed"},{"location":"changelog/#013---2022-05-26","text":"","title":"[0.1.3] - 2022-05-26"},{"location":"changelog/#changed_6","text":"fixed mkdoc rendering of python & markdown.","title":"Changed"},{"location":"changelog/#012---2022-05-26","text":"","title":"[0.1.2] - 2022-05-26"},{"location":"changelog/#changed_7","text":"fix poetry lockfile","title":"Changed"},{"location":"changelog/#011---2022-05-26","text":"","title":"[0.1.1] - 2022-05-26"},{"location":"changelog/#added_6","text":"First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 Environment & Authorisation pre-requisites \u00b6 NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication) toshi-hazard-store (library) \u00b6 To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break store_hazard (script) \u00b6 TODO decribe usage of the upload script","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#environment--authorisation-pre-requisites","text":"NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication)","title":"Environment &amp; Authorisation pre-requisites"},{"location":"usage/#toshi-hazard-store-library","text":"To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break","title":"toshi-hazard-store (library)"},{"location":"usage/#store_hazard-script","text":"TODO decribe usage of the upload script","title":"store_hazard (script)"}]}