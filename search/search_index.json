{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"toshi-hazard-store \u00b6 Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only Features \u00b6 Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#toshi-hazard-store","text":"Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only","title":"toshi-hazard-store"},{"location":"#features","text":"Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for toshi-hazard-store. config \u00b6 This module exports comfiguration for the current system. boolean_env ( environ_name , default = 'FALSE' ) \u00b6 Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ]) model \u00b6 drop_tables () \u00b6 Drop em Source code in toshi_hazard_store/model/__init__.py 26 27 28 29 30 def drop_tables (): \"\"\"Drop em\"\"\" drop_openquake () drop_gridded () drop_disagg () migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 19 20 21 22 23 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_openquake () migrate_gridded () migrate_disagg () attributes \u00b6 attributes \u00b6 This module defines some custom attributes. CompressedJsonicAttribute \u00b6 Bases: Attribute A compressed, json serialisable model attribute Source code in toshi_hazard_store/model/attributes/attributes.py 36 37 38 39 40 41 42 43 44 45 46 47 class CompressedJsonicAttribute ( Attribute ): \"\"\" A compressed, json serialisable model attribute \"\"\" attr_type = STRING def serialize ( self , value : Any ) -> str : return compress_string ( json . dumps ( value )) # could this be pickle?? def deserialize ( self , value : str ) -> Union [ Dict , List ]: return json . loads ( decompress_string ( value )) CompressedListAttribute \u00b6 Bases: CompressedJsonicAttribute A compressed list of floats attribute. Source code in toshi_hazard_store/model/attributes/attributes.py 50 51 52 53 54 55 56 57 58 59 60 61 class CompressedListAttribute ( CompressedJsonicAttribute ): \"\"\" A compressed list of floats attribute. \"\"\" def serialize ( self , value : List [ float ]) -> str : # value = list(value) if value is not None and not isinstance ( value , list ): raise TypeError ( f \"value has invalid type ' { type ( value ) } '; List[float])expected\" , ) return super () . serialize ( value ) CompressedPickleAttribute \u00b6 Bases: Attribute [ bytes ] An attribute containing a binary data object (:code: bytes ) Source code in toshi_hazard_store/model/attributes/attributes.py 64 65 66 67 68 69 70 71 72 73 74 75 class CompressedPickleAttribute ( Attribute [ bytes ]): \"\"\" An attribute containing a binary data object (:code:`bytes`) \"\"\" attr_type = BINARY def serialize ( self , value : bytes ): return zlib . compress ( pickle . dumps ( value )) def deserialize ( self , value : bytes ): return pickle . loads ( zlib . decompress ( value )) IMTValuesAttribute \u00b6 Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/attributes/attributes.py 21 22 23 24 25 26 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute ) LevelValuePairAttribute \u00b6 Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/attributes/attributes.py 29 30 31 32 33 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False ) PickleAttribute \u00b6 Bases: BinaryAttribute This class will serialize/deserialize any picklable Python object. Source code in toshi_hazard_store/model/attributes/attributes.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PickleAttribute ( BinaryAttribute ): \"\"\" This class will serialize/deserialize any picklable Python object. \"\"\" def serialize ( self , value ): \"\"\" The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB \"\"\" return super ( PickleAttribute , self ) . serialize ( pickle . dumps ( value )) def deserialize ( self , value ): return pickle . loads ( super ( PickleAttribute , self ) . deserialize ( value )) serialize ( value ) \u00b6 The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB Source code in toshi_hazard_store/model/attributes/attributes.py 83 84 85 86 87 88 def serialize ( self , value ): \"\"\" The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB \"\"\" return super ( PickleAttribute , self ) . serialize ( pickle . dumps ( value )) enum_attribute \u00b6 This module defines a custom enum attribute. EnumAttribute \u00b6 Bases: Attribute [ T ] Stores names of the supplied Enum as DynamoDB strings. from enum import Enum from pynamodb.models import Model class ShakeFlavor(Enum): VANILLA = 0.1 MINT = 1.22 class Shake(Model): flavor = EnumAttribute(ShakeFlavor) modelB = Shake(flavor=ShakeFlavor.MINT) assert modelB.flavor == ShakeFlavor.MINT Source code in toshi_hazard_store/model/attributes/enum_attribute.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class EnumAttribute ( Attribute [ T ]): \"\"\" Stores names of the supplied Enum as DynamoDB strings. >>> from enum import Enum >>> >>> from pynamodb.models import Model >>> >>> class ShakeFlavor(Enum): >>> VANILLA = 0.1 >>> MINT = 1.22 >>> >>> class Shake(Model): >>> flavor = EnumAttribute(ShakeFlavor) >>> >>> modelB = Shake(flavor=ShakeFlavor.MINT) >>> assert modelB.flavor == ShakeFlavor.MINT \"\"\" attr_type = STRING def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type def deserialize ( self , value : str ) -> Type [ T ]: log . info ( f 'user deserialize value { value } ' ) try : val = self . enum_type [ value ] # getattr(self.enum_type, value) log . info ( f 'enum: { val } ' ) # return val return super () . deserialize ( val ) except ( AttributeError , KeyError ): raise ValueError ( f 'stored value { value } must be a member of { self . enum_type } .' ) def serialize ( self , value : Type [ T ]) -> str : log . info ( f 'user serialize value { value } ' ) if isinstance ( value , self . enum_type ): print ( f 'serialize value { value } ' ) return super () . serialize ( value . name ) else : try : assert self . enum_type ( value ) # CBC MARKS return super () . serialize ( value ) except ( Exception ) as err : print ( err ) raise ValueError ( f 'value { value } must be a member of { self . enum_type } .' ) __init__ ( enum_type , kwargs ) \u00b6 :param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_attribute.py 36 37 38 39 40 41 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type enum_constrained_attribute \u00b6 This module defines some custom enum attributes. EnumConstrainedFloatAttribute \u00b6 Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class EnumConstrainedFloatAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): attr_type = NUMBER value_type = float def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def deserialize ( self , value : Union [ float , int ]) -> float : return float ( super () . deserialize ( float ( value ))) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( float ( value )) __init__ ( enum_type , kwargs ) \u00b6 :param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 122 123 124 125 126 127 128 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) EnumConstrainedIntegerAttribute \u00b6 Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class EnumConstrainedIntegerAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): attr_type = NUMBER value_type = int def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def deserialize ( self , value : Union [ float , int ]) -> int : return int ( super () . deserialize ( int ( value ))) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( int ( value )) __init__ ( enum_type , kwargs ) \u00b6 :param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 96 97 98 99 100 101 102 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) EnumConstrainedUnicodeAttribute \u00b6 Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Stores values of the supplied Unicode Enum as DynamoDB STRING types. Useful where you have values in an existing table field and you want retrofit Enum validation. from enum import Enum from pynamodb.models import Model class ShakeFlavor(Enum): VANILLA = 'vanilla' MINT = 'mint' class Shake(Model): flavor = EnumConstrainedAttribute(ShakeFlavor) modelB = Shake(flavor='mint') Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class EnumConstrainedUnicodeAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): \"\"\" Stores values of the supplied Unicode Enum as DynamoDB STRING types. Useful where you have values in an existing table field and you want retrofit Enum validation. >>> from enum import Enum >>> from pynamodb.models import Model >>> >>> class ShakeFlavor(Enum): >>> VANILLA = 'vanilla' >>> MINT = 'mint' >>> >>> class Shake(Model): >>> flavor = EnumConstrainedAttribute(ShakeFlavor) >>> >>> modelB = Shake(flavor='mint') >>> \"\"\" attr_type = STRING value_type = str def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def deserialize ( self , value : Union [ float , int ]) -> str : return str ( super () . deserialize ( value )) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( value ) __init__ ( enum_type , kwargs ) \u00b6 :param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 77 78 79 80 81 82 83 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) caching \u00b6 cache_store \u00b6 sqlite helpers to manage caching tables cache_enabled () \u00b6 return Ture if the cache is correctly configured. Source code in toshi_hazard_store/model/caching/cache_store.py 138 139 140 141 142 143 144 145 146 147 148 def cache_enabled () -> bool : \"\"\"return Ture if the cache is correctly configured.\"\"\" if LOCAL_CACHE_FOLDER is not None : if pathlib . Path ( LOCAL_CACHE_FOLDER ) . exists (): return True else : log . warning ( f \"Configured cache folder { LOCAL_CACHE_FOLDER } does not exist. Caching is disabled\" ) return False else : log . warning ( \"Local caching is disabled, please check config settings\" ) return False ensure_table_exists ( conn , model_class ) \u00b6 create if needed a cache table for the model_class :param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def ensure_table_exists ( conn : sqlite3 . Connection , model_class : Type [ _T ]): \"\"\"create if needed a cache table for the model_class :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" def create_table_sql ( model_class : Type [ _T ]) -> str : # TEXT, NUMERIC, INTEGER, REAL, BLOB # print(name, _type, _type.attr_type) # print(dir(_type)) type_map = { \"S\" : \"string\" , \"N\" : \"numeric\" , \"L\" : \"string\" } _sql : str = \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % safe_table_name ( model_class ) for name , attr in model_class . get_attributes () . items (): _sql += f ' \\t \" { name } \" { type_map [ attr . attr_type ] } ' if name == model_class . _range_key_attribute () . attr_name : # primary kaye _sql += \" PRIMARY KEY, \\n \" else : _sql += \", \\n \" return f ' { _sql [: - 2 ] } \\n );' create_sql = create_table_sql ( model_class ) print ( create_sql ) try : conn . execute ( create_sql ) except Exception as e : print ( \"EXCEPTION\" , e ) execute_sql ( conn , model_class , sql_statement ) \u00b6 :param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 195 196 197 198 199 200 201 202 203 204 205 def execute_sql ( conn : sqlite3 . Connection , model_class : Type [ _T ], sql_statement : str ): \"\"\" :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" try : res = conn . execute ( sql_statement ) except Exception as e : print ( \"EXCEPTION\" , e ) return res get_model ( conn , model_class , range_key_condition , filter_condition = None ) \u00b6 query cache table and return any hits. :param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def get_model ( conn : sqlite3 . Connection , model_class : Type [ _T ], range_key_condition : Condition , filter_condition : Condition = None ) -> Iterable [ _T ]: \"\"\"query cache table and return any hits. :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" _sql = \"SELECT * FROM %s \\n \" % safe_table_name ( model_class ) # add the compulsary range key _sql += \" \\t WHERE \" + next ( sql_from_pynamodb_condition ( range_key_condition )) # add the optional filter expression if filter_condition is not None : _sql += \" \\n \" for expr in sql_from_pynamodb_condition ( filter_condition ): _sql += f \" \\t AND { expr } \\n \" # print(_sql) try : conn . row_factory = sqlite3 . Row for row in conn . execute ( _sql ): d = dict ( row ) for name , attr in model_class . get_attributes () . items (): # string conversion if attr . attr_type == 'S' : d [ name ] = str ( d [ name ]) # list conversion if attr . attr_type == 'L' : val = base64 . b64decode ( str ( d [ name ])) . decode ( 'ascii' ) d [ name ] = json . loads ( val ) # TODO: this is only good for THS_HAZARDAGGREGATION vals = list () for itm in d [ name ]: # print(itm) vals . append ( LevelValuePairAttribute ( lvl = itm [ 'M' ][ 'lvl' ][ 'N' ], val = itm [ 'M' ][ 'val' ][ 'N' ])) d [ name ] = vals # print('LIST:', name) # print(d[name]) # datetime conversion if isinstance ( attr , TimestampAttribute ): d [ name ] = dt . fromtimestamp ( d [ name ]) . replace ( tzinfo = timezone . utc ) yield model_class ( ** d ) except Exception as e : print ( e ) raise put_model ( conn , model_instance ) \u00b6 write model instance to query cache table. :param conn: Connection object :param model_instance: an instance the model_class :return: None Source code in toshi_hazard_store/model/caching/cache_store.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def put_model ( conn : sqlite3 . Connection , # model_class: Type[_T], model_instance : _T , ): \"\"\"write model instance to query cache table. :param conn: Connection object :param model_instance: an instance the model_class :return: None \"\"\" model_args = model_instance . get_save_kwargs_from_instance ()[ 'Item' ] _sql = \"INSERT INTO %s \\n \" % safe_table_name ( model_instance . __class__ ) # model_class) _sql += \" \\t (\" # attribute names for name in model_instance . get_attributes () . keys (): _sql += f '\" { name } \", ' _sql = _sql [: - 2 ] + \") \\n VALUES ( \\n \" # attrbute values for name , attr in model_instance . get_attributes () . items (): field = model_args . get ( name ) if field is None : # optional fields may not have been set, save `Null` instead _sql += ' \\t Null, \\n ' continue if field . get ( 'S' ): _sql += f ' \\t \" { field [ \"S\" ] } \", \\n ' if field . get ( 'N' ): _sql += f ' \\t { float ( field [ \"N\" ]) } , \\n ' if field . get ( 'L' ): b64_bytes = json . dumps ( field [ \"L\" ]) . encode ( 'ascii' ) _sql += f ' \\t \" { base64 . b64encode ( b64_bytes ) . decode ( \"ascii\" ) } \", \\n ' _sql = _sql [: - 2 ] + \"); \\n \" log . debug ( 'SQL: %s ' % _sql ) try : cursor = conn . cursor () cursor . execute ( _sql ) conn . commit () log . info ( \"Last row id: %s \" % cursor . lastrowid ) # cursor.close() # conn.execute(_sql) except ( sqlite3 . IntegrityError ) as e : msg = str ( e ) if 'UNIQUE constraint failed' in msg : log . debug ( 'attempt to insert a duplicate key failed: ' ) except Exception as e : log . error ( e ) raise sql_from_pynamodb_condition ( condition ) \u00b6 build SQL expression from the pynamodb condition Source code in toshi_hazard_store/model/caching/cache_store.py 241 242 243 244 245 246 247 248 249 250 251 def sql_from_pynamodb_condition ( condition : Condition ) -> Generator : \"\"\"build SQL expression from the pynamodb condition\"\"\" operator = condition . operator # handle nested condition if operator == 'AND' : for cond in condition . values : for expr in sql_from_pynamodb_condition ( cond ): yield expr else : yield _unpack_pynamodb_condition ( condition ) model_cache_mixin \u00b6 This module defines the pynamodb tables used to store openquake data. Third iteration ModelCacheMixin \u00b6 Bases: pynamodb . models . Model extends pynamodb.models.Model with a local read-through cache for the user model. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class ModelCacheMixin ( pynamodb . models . Model ): \"\"\"extends pynamodb.models.Model with a local read-through cache for the user model.\"\"\" @classmethod def query ( # type: ignore cls : Type [ _T ], hash_key : _KeyType , range_key_condition : Optional [ Condition ] = None , filter_condition : Optional [ Condition ] = None , consistent_read : bool = False , index_name : Optional [ str ] = None , scan_index_forward : Optional [ bool ] = None , limit : Optional [ int ] = None , last_evaluated_key : Optional [ Dict [ str , Dict [ str , Any ]]] = None , attributes_to_get : Optional [ Iterable [ str ]] = None , page_size : Optional [ int ] = None , rate_limit : Optional [ float ] = None , settings : OperationSettings = OperationSettings . default , ) -> pynamodb . models . ResultIterator [ _T ]: # \"\"\" Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb \"\"\" # CBC TODO support optional filter condition if supplied range_condition operand is \"=\" if ( not cache_store . cache_enabled ()) and ( filter_condition is not None ): log . warning ( \"Not using the cache\" ) return super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ) log . info ( 'Try the local_cache first' ) if isinstance ( filter_condition , Condition ): conn = cache_store . get_connection ( model_class = cls ) cached_rows = list ( cache_store . get_model ( conn , cls , range_key_condition , filter_condition )) # type: ignore minimum_expected_hits = cache_store . count_permutations ( filter_condition ) log . info ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) if len ( cached_rows ) >= minimum_expected_hits : return cached_rows # type: ignore if len ( cached_rows ) < minimum_expected_hits : log . warn ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) result = [] for res in super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ): cache_store . put_model ( conn , res ) result . append ( res ) return result # type: ignore @classmethod def create_table ( cls : Type [ _T ], wait : bool = False , read_capacity_units : Optional [ int ] = None , write_capacity_units : Optional [ int ] = None , billing_mode : Optional [ str ] = None , ignore_update_ttl_errors : bool = False , ): \"\"\" extends create_table to manage the local_cache table. \"\"\" if cache_store . cache_enabled (): log . info ( \"setup local cache\" ) conn = cache_store . get_connection ( model_class = cls ) cache_store . ensure_table_exists ( conn , model_class = cls ) return super () . create_table ( # type: ignore wait , read_capacity_units , write_capacity_units , billing_mode , ignore_update_ttl_errors , ) @classmethod def delete_table ( cls : Type [ _T ]): \"\"\" extends delete_table to manage the local_cache table. \"\"\" log . info ( 'drop the table ' ) return super () . delete_table () # type: ignore create_table ( wait = False , read_capacity_units = None , write_capacity_units = None , billing_mode = None , ignore_update_ttl_errors = False ) classmethod \u00b6 extends create_table to manage the local_cache table. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @classmethod def create_table ( cls : Type [ _T ], wait : bool = False , read_capacity_units : Optional [ int ] = None , write_capacity_units : Optional [ int ] = None , billing_mode : Optional [ str ] = None , ignore_update_ttl_errors : bool = False , ): \"\"\" extends create_table to manage the local_cache table. \"\"\" if cache_store . cache_enabled (): log . info ( \"setup local cache\" ) conn = cache_store . get_connection ( model_class = cls ) cache_store . ensure_table_exists ( conn , model_class = cls ) return super () . create_table ( # type: ignore wait , read_capacity_units , write_capacity_units , billing_mode , ignore_update_ttl_errors , ) delete_table () classmethod \u00b6 extends delete_table to manage the local_cache table. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 116 117 118 119 120 121 122 @classmethod def delete_table ( cls : Type [ _T ]): \"\"\" extends delete_table to manage the local_cache table. \"\"\" log . info ( 'drop the table ' ) return super () . delete_table () # type: ignore query ( hash_key , range_key_condition = None , filter_condition = None , consistent_read = False , index_name = None , scan_index_forward = None , limit = None , last_evaluated_key = None , attributes_to_get = None , page_size = None , rate_limit = None , settings = OperationSettings . default ) classmethod \u00b6 Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @classmethod def query ( # type: ignore cls : Type [ _T ], hash_key : _KeyType , range_key_condition : Optional [ Condition ] = None , filter_condition : Optional [ Condition ] = None , consistent_read : bool = False , index_name : Optional [ str ] = None , scan_index_forward : Optional [ bool ] = None , limit : Optional [ int ] = None , last_evaluated_key : Optional [ Dict [ str , Dict [ str , Any ]]] = None , attributes_to_get : Optional [ Iterable [ str ]] = None , page_size : Optional [ int ] = None , rate_limit : Optional [ float ] = None , settings : OperationSettings = OperationSettings . default , ) -> pynamodb . models . ResultIterator [ _T ]: # \"\"\" Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb \"\"\" # CBC TODO support optional filter condition if supplied range_condition operand is \"=\" if ( not cache_store . cache_enabled ()) and ( filter_condition is not None ): log . warning ( \"Not using the cache\" ) return super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ) log . info ( 'Try the local_cache first' ) if isinstance ( filter_condition , Condition ): conn = cache_store . get_connection ( model_class = cls ) cached_rows = list ( cache_store . get_model ( conn , cls , range_key_condition , filter_condition )) # type: ignore minimum_expected_hits = cache_store . count_permutations ( filter_condition ) log . info ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) if len ( cached_rows ) >= minimum_expected_hits : return cached_rows # type: ignore if len ( cached_rows ) < minimum_expected_hits : log . warn ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) result = [] for res in super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ): cache_store . put_model ( conn , res ) result . append ( res ) return result # type: ignore constraints \u00b6 AggregationEnum \u00b6 Bases: Enum Defines the values available for aggregations. Source code in toshi_hazard_store/model/constraints.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class AggregationEnum ( Enum ): \"\"\"Defines the values available for aggregations.\"\"\" MEAN = 'mean' COV = 'cov' STD = 'std' _005 = '0.005' _01 = '0.01' _025 = '0.025' _05 = '0.05' _10 = '0.1' _20 = '0.2' _30 = '0.3' _40 = '0.4' _50 = '0.5' _60 = '0.6' _70 = '0.7' _80 = '0.8' _90 = '0.9' _95 = '0.95' _975 = '0.975' _99 = '0.99' _995 = '0.995' IntensityMeasureTypeEnum \u00b6 Bases: Enum Defines the values available for IMTs. Source code in toshi_hazard_store/model/constraints.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class IntensityMeasureTypeEnum ( Enum ): \"\"\" Defines the values available for IMTs. \"\"\" PGA = 'PGA' SA_0_1 = 'SA(0.1)' SA_0_15 = 'SA(0.15)' SA_0_2 = 'SA(0.2)' SA_0_25 = 'SA(0.25)' SA_0_3 = 'SA(0.3)' SA_0_35 = 'SA(0.35)' SA_0_4 = 'SA(0.4)' SA_0_5 = 'SA(0.5)' SA_0_6 = 'SA(0.6)' SA_0_7 = 'SA(0.7)' SA_0_8 = 'SA(0.8)' SA_0_9 = 'SA(0.9)' SA_1_0 = 'SA(1.0)' SA_1_25 = 'SA(1.25)' SA_1_5 = 'SA(1.5)' SA_1_75 = 'SA(1.75)' SA_2_0 = 'SA(2.0)' SA_2_5 = 'SA(2.5)' SA_3_0 = 'SA(3.0)' SA_3_5 = 'SA(3.5)' SA_4_0 = 'SA(4.0)' SA_4_5 = 'SA(4.5)' SA_5_0 = 'SA(5.0)' SA_6_0 = 'SA(6.0)' SA_7_5 = 'SA(7.5)' SA_10_0 = 'SA(10.0)' ProbabilityEnum \u00b6 Bases: Enum Defines the values available for probabilities. store values as float representing probability in 1 year Source code in toshi_hazard_store/model/constraints.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class ProbabilityEnum ( Enum ): \"\"\" Defines the values available for probabilities. store values as float representing probability in 1 year \"\"\" _86_PCT_IN_50YRS = 3.8559e-02 _63_PCT_IN_50YRS = 1.9689e-02 _39_PCT_IN_50YRS = 9.8372e-03 _18_PCT_IN_50YRS = 3.9612e-03 _10_PCT_IN_50YRS = 2.1050e-03 _5_PCT_IN_50YRS = 1.0253e-03 _2_PCT_IN_50YRS = 4.0397e-04 _1_PCT_IN_50YRS = 2.0099e-04 VS30Enum \u00b6 Bases: Enum Defines the values available for VS30. Source code in toshi_hazard_store/model/constraints.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class VS30Enum ( Enum ): \"\"\" Defines the values available for VS30. \"\"\" _0 = 0 # indicates that this value is not used _150 = 150 _175 = 175 _200 = 200 _225 = 225 _250 = 250 _275 = 275 _300 = 300 _350 = 350 _375 = 375 _400 = 400 _450 = 450 _500 = 500 _525 = 525 _550 = 550 _600 = 600 _650 = 650 _700 = 700 _750 = 750 _800 = 800 _850 = 850 _900 = 900 _950 = 950 _1000 = 1000 _1050 = 1050 _1100 = 1100 _1500 = 1500 disagg_models \u00b6 This module defines the pynamodb tables used to store openquake data. Third iteration DisaggAggregationBase \u00b6 Bases: LocationIndexedModel Store aggregated disaggregations. Source code in toshi_hazard_store/model/disagg_models.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class DisaggAggregationBase ( LocationIndexedModel ): \"\"\"Store aggregated disaggregations.\"\"\" hazard_model_id = UnicodeAttribute () imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) hazard_agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) # eg MEAN disagg_agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) disaggs = CompressedPickleAttribute () # a very compressible numpy array, bins = PickleAttribute () # a much smaller numpy array shaking_level = FloatAttribute () probability = EnumAttribute ( ProbabilityEnum ) # eg TEN_PCT_IN_50YRS def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = ( f ' { self . hazard_model_id } : { self . hazard_agg } : { self . disagg_agg } :' f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . probability . name } ' ) return self set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/disagg_models.py 41 42 43 44 45 46 47 48 49 50 51 52 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = ( f ' { self . hazard_model_id } : { self . hazard_agg } : { self . disagg_agg } :' f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . probability . name } ' ) return self drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/disagg_models.py 116 117 118 119 120 121 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/disagg_models.py 107 108 109 110 111 112 113 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) gridded_hazard \u00b6 This module defines the pynamodb tables used to store THH. GriddedHazard \u00b6 Bases: Model Grid points defined in location_grid_id has a values in grid_poes. Source code in toshi_hazard_store/model/gridded_hazard.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class GriddedHazard ( Model ): \"\"\"Grid points defined in location_grid_id has a values in grid_poes.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) sort_key = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_model_id = UnicodeAttribute () location_grid_id = UnicodeAttribute () vs30 = EnumConstrainedIntegerAttribute ( VS30Enum ) imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) poe = FloatAttribute () grid_poes = CompressedListAttribute () @staticmethod def new_model ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe , grid_poes ) -> 'GriddedHazard' : obj = GriddedHazard ( hazard_model_id = hazard_model_id , location_grid_id = location_grid_id , vs30 = vs30 , imt = imt , agg = agg , poe = poe , grid_poes = grid_poes , ) obj . partition_key = f \" { obj . hazard_model_id } \" obj . sort_key = f \" { obj . hazard_model_id } : { obj . location_grid_id } : { obj . vs30 } : { obj . imt } : { obj . agg } : { obj . poe } \" return obj Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/gridded_hazard.py 26 27 28 29 30 31 32 33 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/gridded_hazard.py 79 80 81 82 83 84 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/gridded_hazard.py 70 71 72 73 74 75 76 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) location_indexed_model \u00b6 LocationIndexedModel \u00b6 Bases: Model Model base class. Source code in toshi_hazard_store/model/location_indexed_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = EnumConstrainedIntegerAttribute ( VS30Enum ) site_vs30 = FloatAttribute ( null = True ) created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/location_indexed_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self openquake_models \u00b6 This module defines the pynamodb tables used to store openquake data. Third iteration HazardAggregation \u00b6 Bases: ModelCacheMixin , LocationIndexedModel A pynamodb model for aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_models.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class HazardAggregation ( ModelCacheMixin , LocationIndexedModel ): \"\"\"A pynamodb model for aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) values = ListAttribute ( of = LevelValuePairAttribute ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1 Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 92 93 94 95 96 97 98 99 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_models.py 107 108 109 110 111 112 113 114 115 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self to_csv ( models ) staticmethod \u00b6 Generate lists ready for csv module - including a header, followed by n rows. Source code in toshi_hazard_store/model/openquake_models.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1 OpenquakeRealization \u00b6 Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 150 151 152 153 154 155 156 157 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_models.py 170 171 172 173 174 175 176 177 178 179 180 181 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self ToshiOpenquakeMeta \u00b6 Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_models.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 32 33 34 35 36 37 38 39 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover vs30_nloc001_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_models.py 76 77 78 79 80 81 82 83 84 85 86 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True ) vs30_nloc1_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_models.py 63 64 65 66 67 68 69 70 71 72 73 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True ) drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_models.py 199 200 201 202 203 204 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_models.py 191 192 193 194 195 196 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) log . info ( f \"Migrate created table: { table } \" ) multi_batch \u00b6 DynamoBatchWorker \u00b6 Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") # if self.model == model.ToshiOpenquakeHazardCurveStatsV2: # query.batch_save_hcurve_stats_v2(self.toshi_id, models=models) # elif self.model == model.ToshiOpenquakeHazardCurveRlzsV2: # query.batch_save_hcurve_rlzs_v2(self.toshi_id, models=models) if self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" ) oq_import \u00b6 export_v3 \u00b6 export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ) \u00b6 Extract and same the meta data. Source code in toshi_hazard_store/oq_import/export_v3.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) vs30 = oq . reference_vs30_value if math . isnan ( vs30 ): vs30 = 0 obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( vs30 )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( vs30 ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj ) pynamodb_settings \u00b6 pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered. query \u00b6 disagg_queries \u00b6 Queries for saving and retrieving gridded hazard convenience. get_one_disagg_aggregation ( hazard_model_id , hazard_agg , disagg_agg , location , vs30 , imt , poe , model = mDAE ) \u00b6 Fetch model based on single model arguments. Source code in toshi_hazard_store/query/disagg_queries.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_one_disagg_aggregation ( hazard_model_id : str , hazard_agg : AggregationEnum , disagg_agg : AggregationEnum , location : CodedLocation , vs30 : float , imt : str , poe : ProbabilityEnum , model : Type [ Union [ mDAE , mDAO ]] = mDAE , ) -> Union [ mDAE , mDAO , None ]: \"\"\"Fetch model based on single model arguments.\"\"\" qry = model . query ( downsample_code ( location , 0.1 ), range_key_condition = model . sort_key == f ' { hazard_model_id } : { hazard_agg . value } : { disagg_agg . value } :' f ' { location } : { vs30 } : { imt } : { poe . name } ' , # type: ignore ) log . debug ( f \"get_one_disagg_aggregation: qry { qry } \" ) result : List [ Union [ mDAE , mDAO ]] = list ( qry ) assert len ( result ) in [ 0 , 1 ] if len ( result ): return result [ 0 ] return None gridded_hazard_query \u00b6 Queries for saving and retrieving gridded hazard convenience. get_gridded_hazard ( hazard_model_ids , location_grid_ids , vs30s , imts , aggs , poes ) \u00b6 Fetch GriddedHazard based on criteria. Source code in toshi_hazard_store/query/gridded_hazard_query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_gridded_hazard ( hazard_model_ids : Iterable [ str ], location_grid_ids : Iterable [ str ], vs30s : Iterable [ float ], imts : Iterable [ str ], aggs : Iterable [ str ], poes : Iterable [ float ], ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on criteria.\"\"\" # partition_key = f\"{obj.hazard_model_id}\" # sort_key = f\"{obj.hazard_model_id}:{obj.location_grid_id}:{obj.vs30}:{obj.imt}:{obj.agg}:{obj.poe}\" def build_sort_key ( hazard_model_id , grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build sort_key.\"\"\" sort_key = hazard_model_id sort_key = sort_key + f \": { sorted ( grid_ids )[ 0 ] } \" if grid_ids else sort_key sort_key = sort_key + f \": { sorted ( vs30s )[ 0 ] } \" if grid_ids and vs30s else sort_key sort_key = sort_key + f \": { sorted ( imts )[ 0 ] } \" if grid_ids and vs30s and imts else sort_key sort_key = sort_key + f \": { sorted ( aggs )[ 0 ] } \" if grid_ids and vs30s and imts and aggs else sort_key sort_key = sort_key + f \": { sorted ( poes )[ 0 ] } \" if grid_ids and vs30s and imts and aggs and poes else sort_key return sort_key def build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build filter condition.\"\"\" condition_expr = mGH . hazard_model_id == hazard_model_id if location_grid_ids : condition_expr = condition_expr & mGH . location_grid_id . is_in ( * location_grid_ids ) if vs30s : condition_expr = condition_expr & mGH . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mGH . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mGH . agg . is_in ( * aggs ) if poes : condition_expr = condition_expr & mGH . poe . is_in ( * poes ) return condition_expr # TODO: this can be parallelised/optimised. for hazard_model_id in hazard_model_ids : sort_key_first_val = build_sort_key ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) condition_expr = build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) log . debug ( f 'sort_key_first_val { sort_key_first_val } ' ) log . debug ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mGH . query ( hazard_model_id , mGH . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mGH . query ( hazard_model_id , mGH . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit ) get_one_gridded_hazard ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe ) \u00b6 Fetch GriddedHazard based on single criteria. Source code in toshi_hazard_store/query/gridded_hazard_query.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_one_gridded_hazard ( hazard_model_id : str , location_grid_id : str , vs30 : float , imt : str , agg : str , poe : float , ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on single criteria.\"\"\" qry = mGH . query ( hazard_model_id , mGH . sort_key == f ' { hazard_model_id } : { location_grid_id } : { vs30 } : { imt } : { agg } : { poe } ' ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit ) hazard_query \u00b6 Queries for saving and retrieving openquake hazard results with convenience. first_vs30_key ( vs30s ) \u00b6 This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. Source code in toshi_hazard_store/query/hazard_query.py 61 62 63 64 65 66 67 68 69 70 71 def first_vs30_key ( vs30s ): \"\"\"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. \"\"\" if have_mixed_length_vs30s ( vs30s ): vsLong = filter ( lambda x : x >= 1000 , vs30s ) return str ( int ( min ( vsLong ) / 10 )) . zfill ( model . VS30_KEYLEN ) return str ( min ( vs30s )) . zfill ( model . VS30_KEYLEN ) get_hazard_curves ( locs = [], vs30s = [], hazard_model_ids = [], imts = [], aggs = [], local_cache = False ) \u00b6 Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query/hazard_query.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], local_cache : bool = False , ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): log . info ( 'hash_key %s ' % hash_location_code ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) log . debug ( 'sort_key_first_val: %s ' % sort_key_first_val ) log . debug ( 'condition_expr: %s ' % condition_expr ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( \"get_hazard_rlz_curves_v3: qry %s \" % qry ) hits = 0 for hit in qry : hits += 1 yield ( hit ) log . info ( 'hash_key %s has %s hits' % ( hash_location_code , hits )) get_hazard_metadata_v3 ( haz_sol_ids = None , vs30_vals = None ) \u00b6 Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query/hazard_query.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit ) get_rlz_curves_v3 ( locs = [], vs30s = [], rlzs = [], tids = [], imts = []) \u00b6 Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query/hazard_query.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit ) have_mixed_length_vs30s ( vs30s ) \u00b6 Does the list of vs30s require mixed length index keys. Source code in toshi_hazard_store/query/hazard_query.py 50 51 52 53 54 55 56 57 58 def have_mixed_length_vs30s ( vs30s ): \"\"\"Does the list of vs30s require mixed length index keys.\"\"\" max_vs30 = max ( vs30s ) min_vs30 = min ( vs30s ) if max_vs30 >= 1000 : if min_vs30 < 1000 : return True else : return False transform \u00b6 Helper functions to export an openquake calculation and save it with toshi-hazard-store. parse_logic_tree_branches ( file_id ) \u00b6 Extract the dataframes. Source code in toshi_hazard_store/transform.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt utils \u00b6 Common utilities. normalise_site_code ( oq_site_object , force_normalized = False ) \u00b6 Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"Modules"},{"location":"api/#toshi_hazard_store.config","text":"This module exports comfiguration for the current system.","title":"config"},{"location":"api/#toshi_hazard_store.config.boolean_env","text":"Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ])","title":"boolean_env()"},{"location":"api/#toshi_hazard_store.model","text":"","title":"model"},{"location":"api/#toshi_hazard_store.model.drop_tables","text":"Drop em Source code in toshi_hazard_store/model/__init__.py 26 27 28 29 30 def drop_tables (): \"\"\"Drop em\"\"\" drop_openquake () drop_gridded () drop_disagg ()","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 19 20 21 22 23 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_openquake () migrate_gridded () migrate_disagg ()","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.attributes","text":"","title":"attributes"},{"location":"api/#toshi_hazard_store.model.attributes.attributes","text":"This module defines some custom attributes.","title":"attributes"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.CompressedJsonicAttribute","text":"Bases: Attribute A compressed, json serialisable model attribute Source code in toshi_hazard_store/model/attributes/attributes.py 36 37 38 39 40 41 42 43 44 45 46 47 class CompressedJsonicAttribute ( Attribute ): \"\"\" A compressed, json serialisable model attribute \"\"\" attr_type = STRING def serialize ( self , value : Any ) -> str : return compress_string ( json . dumps ( value )) # could this be pickle?? def deserialize ( self , value : str ) -> Union [ Dict , List ]: return json . loads ( decompress_string ( value ))","title":"CompressedJsonicAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.CompressedListAttribute","text":"Bases: CompressedJsonicAttribute A compressed list of floats attribute. Source code in toshi_hazard_store/model/attributes/attributes.py 50 51 52 53 54 55 56 57 58 59 60 61 class CompressedListAttribute ( CompressedJsonicAttribute ): \"\"\" A compressed list of floats attribute. \"\"\" def serialize ( self , value : List [ float ]) -> str : # value = list(value) if value is not None and not isinstance ( value , list ): raise TypeError ( f \"value has invalid type ' { type ( value ) } '; List[float])expected\" , ) return super () . serialize ( value )","title":"CompressedListAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.CompressedPickleAttribute","text":"Bases: Attribute [ bytes ] An attribute containing a binary data object (:code: bytes ) Source code in toshi_hazard_store/model/attributes/attributes.py 64 65 66 67 68 69 70 71 72 73 74 75 class CompressedPickleAttribute ( Attribute [ bytes ]): \"\"\" An attribute containing a binary data object (:code:`bytes`) \"\"\" attr_type = BINARY def serialize ( self , value : bytes ): return zlib . compress ( pickle . dumps ( value )) def deserialize ( self , value : bytes ): return pickle . loads ( zlib . decompress ( value ))","title":"CompressedPickleAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.IMTValuesAttribute","text":"Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/attributes/attributes.py 21 22 23 24 25 26 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute )","title":"IMTValuesAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.LevelValuePairAttribute","text":"Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/attributes/attributes.py 29 30 31 32 33 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False )","title":"LevelValuePairAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.PickleAttribute","text":"Bases: BinaryAttribute This class will serialize/deserialize any picklable Python object. Source code in toshi_hazard_store/model/attributes/attributes.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PickleAttribute ( BinaryAttribute ): \"\"\" This class will serialize/deserialize any picklable Python object. \"\"\" def serialize ( self , value ): \"\"\" The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB \"\"\" return super ( PickleAttribute , self ) . serialize ( pickle . dumps ( value )) def deserialize ( self , value ): return pickle . loads ( super ( PickleAttribute , self ) . deserialize ( value ))","title":"PickleAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.attributes.PickleAttribute.serialize","text":"The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB Source code in toshi_hazard_store/model/attributes/attributes.py 83 84 85 86 87 88 def serialize ( self , value ): \"\"\" The super class takes the binary string returned from pickle.dumps and encodes it for storage in DynamoDB \"\"\" return super ( PickleAttribute , self ) . serialize ( pickle . dumps ( value ))","title":"serialize()"},{"location":"api/#toshi_hazard_store.model.attributes.enum_attribute","text":"This module defines a custom enum attribute.","title":"enum_attribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_attribute.EnumAttribute","text":"Bases: Attribute [ T ] Stores names of the supplied Enum as DynamoDB strings. from enum import Enum from pynamodb.models import Model class ShakeFlavor(Enum): VANILLA = 0.1 MINT = 1.22 class Shake(Model): flavor = EnumAttribute(ShakeFlavor) modelB = Shake(flavor=ShakeFlavor.MINT) assert modelB.flavor == ShakeFlavor.MINT Source code in toshi_hazard_store/model/attributes/enum_attribute.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class EnumAttribute ( Attribute [ T ]): \"\"\" Stores names of the supplied Enum as DynamoDB strings. >>> from enum import Enum >>> >>> from pynamodb.models import Model >>> >>> class ShakeFlavor(Enum): >>> VANILLA = 0.1 >>> MINT = 1.22 >>> >>> class Shake(Model): >>> flavor = EnumAttribute(ShakeFlavor) >>> >>> modelB = Shake(flavor=ShakeFlavor.MINT) >>> assert modelB.flavor == ShakeFlavor.MINT \"\"\" attr_type = STRING def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type def deserialize ( self , value : str ) -> Type [ T ]: log . info ( f 'user deserialize value { value } ' ) try : val = self . enum_type [ value ] # getattr(self.enum_type, value) log . info ( f 'enum: { val } ' ) # return val return super () . deserialize ( val ) except ( AttributeError , KeyError ): raise ValueError ( f 'stored value { value } must be a member of { self . enum_type } .' ) def serialize ( self , value : Type [ T ]) -> str : log . info ( f 'user serialize value { value } ' ) if isinstance ( value , self . enum_type ): print ( f 'serialize value { value } ' ) return super () . serialize ( value . name ) else : try : assert self . enum_type ( value ) # CBC MARKS return super () . serialize ( value ) except ( Exception ) as err : print ( err ) raise ValueError ( f 'value { value } must be a member of { self . enum_type } .' )","title":"EnumAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_attribute.EnumAttribute.__init__","text":":param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_attribute.py 36 37 38 39 40 41 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type","title":"__init__()"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute","text":"This module defines some custom enum attributes.","title":"enum_constrained_attribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedFloatAttribute","text":"Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class EnumConstrainedFloatAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): attr_type = NUMBER value_type = float def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def deserialize ( self , value : Union [ float , int ]) -> float : return float ( super () . deserialize ( float ( value ))) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( float ( value ))","title":"EnumConstrainedFloatAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedFloatAttribute.__init__","text":":param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 122 123 124 125 126 127 128 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type )","title":"__init__()"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedIntegerAttribute","text":"Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class EnumConstrainedIntegerAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): attr_type = NUMBER value_type = int def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def deserialize ( self , value : Union [ float , int ]) -> int : return int ( super () . deserialize ( int ( value ))) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( int ( value ))","title":"EnumConstrainedIntegerAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedIntegerAttribute.__init__","text":":param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 96 97 98 99 100 101 102 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type )","title":"__init__()"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedUnicodeAttribute","text":"Bases: EnumConstrainedAttributeMixin , Attribute [ T ] Stores values of the supplied Unicode Enum as DynamoDB STRING types. Useful where you have values in an existing table field and you want retrofit Enum validation. from enum import Enum from pynamodb.models import Model class ShakeFlavor(Enum): VANILLA = 'vanilla' MINT = 'mint' class Shake(Model): flavor = EnumConstrainedAttribute(ShakeFlavor) modelB = Shake(flavor='mint') Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class EnumConstrainedUnicodeAttribute ( EnumConstrainedAttributeMixin , Attribute [ T ]): \"\"\" Stores values of the supplied Unicode Enum as DynamoDB STRING types. Useful where you have values in an existing table field and you want retrofit Enum validation. >>> from enum import Enum >>> from pynamodb.models import Model >>> >>> class ShakeFlavor(Enum): >>> VANILLA = 'vanilla' >>> MINT = 'mint' >>> >>> class Shake(Model): >>> flavor = EnumConstrainedAttribute(ShakeFlavor) >>> >>> modelB = Shake(flavor='mint') >>> \"\"\" attr_type = STRING value_type = str def __set__ ( self , instance : Any , value : Optional [ T ]) -> None : if isinstance ( value , self . enum_type ): log . info ( f 'user __set__ enum type { value } { str ( value . value ) } ' ) super () . __set__ ( instance , value . value ) else : super () . __set__ ( instance , value ) def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type ) def deserialize ( self , value : Union [ float , int ]) -> str : return str ( super () . deserialize ( value )) def serialize ( self , value : Union [ float , int ]) -> str : return super () . serialize ( value )","title":"EnumConstrainedUnicodeAttribute"},{"location":"api/#toshi_hazard_store.model.attributes.enum_constrained_attribute.EnumConstrainedUnicodeAttribute.__init__","text":":param enum_type: The type of the enum Source code in toshi_hazard_store/model/attributes/enum_constrained_attribute.py 77 78 79 80 81 82 83 def __init__ ( self , enum_type : Type [ T ], ** kwargs : Any ) -> None : \"\"\" :param enum_type: The type of the enum \"\"\" super () . __init__ ( ** kwargs ) self . enum_type = enum_type super () . _validate_enum ( self . enum_type , self . value_type )","title":"__init__()"},{"location":"api/#toshi_hazard_store.model.caching","text":"","title":"caching"},{"location":"api/#toshi_hazard_store.model.caching.cache_store","text":"sqlite helpers to manage caching tables","title":"cache_store"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.cache_enabled","text":"return Ture if the cache is correctly configured. Source code in toshi_hazard_store/model/caching/cache_store.py 138 139 140 141 142 143 144 145 146 147 148 def cache_enabled () -> bool : \"\"\"return Ture if the cache is correctly configured.\"\"\" if LOCAL_CACHE_FOLDER is not None : if pathlib . Path ( LOCAL_CACHE_FOLDER ) . exists (): return True else : log . warning ( f \"Configured cache folder { LOCAL_CACHE_FOLDER } does not exist. Caching is disabled\" ) return False else : log . warning ( \"Local caching is disabled, please check config settings\" ) return False","title":"cache_enabled()"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.ensure_table_exists","text":"create if needed a cache table for the model_class :param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def ensure_table_exists ( conn : sqlite3 . Connection , model_class : Type [ _T ]): \"\"\"create if needed a cache table for the model_class :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" def create_table_sql ( model_class : Type [ _T ]) -> str : # TEXT, NUMERIC, INTEGER, REAL, BLOB # print(name, _type, _type.attr_type) # print(dir(_type)) type_map = { \"S\" : \"string\" , \"N\" : \"numeric\" , \"L\" : \"string\" } _sql : str = \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % safe_table_name ( model_class ) for name , attr in model_class . get_attributes () . items (): _sql += f ' \\t \" { name } \" { type_map [ attr . attr_type ] } ' if name == model_class . _range_key_attribute () . attr_name : # primary kaye _sql += \" PRIMARY KEY, \\n \" else : _sql += \", \\n \" return f ' { _sql [: - 2 ] } \\n );' create_sql = create_table_sql ( model_class ) print ( create_sql ) try : conn . execute ( create_sql ) except Exception as e : print ( \"EXCEPTION\" , e )","title":"ensure_table_exists()"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.execute_sql","text":":param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 195 196 197 198 199 200 201 202 203 204 205 def execute_sql ( conn : sqlite3 . Connection , model_class : Type [ _T ], sql_statement : str ): \"\"\" :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" try : res = conn . execute ( sql_statement ) except Exception as e : print ( \"EXCEPTION\" , e ) return res","title":"execute_sql()"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.get_model","text":"query cache table and return any hits. :param conn: Connection object :param model_class: type of the model_class :return: Source code in toshi_hazard_store/model/caching/cache_store.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def get_model ( conn : sqlite3 . Connection , model_class : Type [ _T ], range_key_condition : Condition , filter_condition : Condition = None ) -> Iterable [ _T ]: \"\"\"query cache table and return any hits. :param conn: Connection object :param model_class: type of the model_class :return: \"\"\" _sql = \"SELECT * FROM %s \\n \" % safe_table_name ( model_class ) # add the compulsary range key _sql += \" \\t WHERE \" + next ( sql_from_pynamodb_condition ( range_key_condition )) # add the optional filter expression if filter_condition is not None : _sql += \" \\n \" for expr in sql_from_pynamodb_condition ( filter_condition ): _sql += f \" \\t AND { expr } \\n \" # print(_sql) try : conn . row_factory = sqlite3 . Row for row in conn . execute ( _sql ): d = dict ( row ) for name , attr in model_class . get_attributes () . items (): # string conversion if attr . attr_type == 'S' : d [ name ] = str ( d [ name ]) # list conversion if attr . attr_type == 'L' : val = base64 . b64decode ( str ( d [ name ])) . decode ( 'ascii' ) d [ name ] = json . loads ( val ) # TODO: this is only good for THS_HAZARDAGGREGATION vals = list () for itm in d [ name ]: # print(itm) vals . append ( LevelValuePairAttribute ( lvl = itm [ 'M' ][ 'lvl' ][ 'N' ], val = itm [ 'M' ][ 'val' ][ 'N' ])) d [ name ] = vals # print('LIST:', name) # print(d[name]) # datetime conversion if isinstance ( attr , TimestampAttribute ): d [ name ] = dt . fromtimestamp ( d [ name ]) . replace ( tzinfo = timezone . utc ) yield model_class ( ** d ) except Exception as e : print ( e ) raise","title":"get_model()"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.put_model","text":"write model instance to query cache table. :param conn: Connection object :param model_instance: an instance the model_class :return: None Source code in toshi_hazard_store/model/caching/cache_store.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def put_model ( conn : sqlite3 . Connection , # model_class: Type[_T], model_instance : _T , ): \"\"\"write model instance to query cache table. :param conn: Connection object :param model_instance: an instance the model_class :return: None \"\"\" model_args = model_instance . get_save_kwargs_from_instance ()[ 'Item' ] _sql = \"INSERT INTO %s \\n \" % safe_table_name ( model_instance . __class__ ) # model_class) _sql += \" \\t (\" # attribute names for name in model_instance . get_attributes () . keys (): _sql += f '\" { name } \", ' _sql = _sql [: - 2 ] + \") \\n VALUES ( \\n \" # attrbute values for name , attr in model_instance . get_attributes () . items (): field = model_args . get ( name ) if field is None : # optional fields may not have been set, save `Null` instead _sql += ' \\t Null, \\n ' continue if field . get ( 'S' ): _sql += f ' \\t \" { field [ \"S\" ] } \", \\n ' if field . get ( 'N' ): _sql += f ' \\t { float ( field [ \"N\" ]) } , \\n ' if field . get ( 'L' ): b64_bytes = json . dumps ( field [ \"L\" ]) . encode ( 'ascii' ) _sql += f ' \\t \" { base64 . b64encode ( b64_bytes ) . decode ( \"ascii\" ) } \", \\n ' _sql = _sql [: - 2 ] + \"); \\n \" log . debug ( 'SQL: %s ' % _sql ) try : cursor = conn . cursor () cursor . execute ( _sql ) conn . commit () log . info ( \"Last row id: %s \" % cursor . lastrowid ) # cursor.close() # conn.execute(_sql) except ( sqlite3 . IntegrityError ) as e : msg = str ( e ) if 'UNIQUE constraint failed' in msg : log . debug ( 'attempt to insert a duplicate key failed: ' ) except Exception as e : log . error ( e ) raise","title":"put_model()"},{"location":"api/#toshi_hazard_store.model.caching.cache_store.sql_from_pynamodb_condition","text":"build SQL expression from the pynamodb condition Source code in toshi_hazard_store/model/caching/cache_store.py 241 242 243 244 245 246 247 248 249 250 251 def sql_from_pynamodb_condition ( condition : Condition ) -> Generator : \"\"\"build SQL expression from the pynamodb condition\"\"\" operator = condition . operator # handle nested condition if operator == 'AND' : for cond in condition . values : for expr in sql_from_pynamodb_condition ( cond ): yield expr else : yield _unpack_pynamodb_condition ( condition )","title":"sql_from_pynamodb_condition()"},{"location":"api/#toshi_hazard_store.model.caching.model_cache_mixin","text":"This module defines the pynamodb tables used to store openquake data. Third iteration","title":"model_cache_mixin"},{"location":"api/#toshi_hazard_store.model.caching.model_cache_mixin.ModelCacheMixin","text":"Bases: pynamodb . models . Model extends pynamodb.models.Model with a local read-through cache for the user model. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class ModelCacheMixin ( pynamodb . models . Model ): \"\"\"extends pynamodb.models.Model with a local read-through cache for the user model.\"\"\" @classmethod def query ( # type: ignore cls : Type [ _T ], hash_key : _KeyType , range_key_condition : Optional [ Condition ] = None , filter_condition : Optional [ Condition ] = None , consistent_read : bool = False , index_name : Optional [ str ] = None , scan_index_forward : Optional [ bool ] = None , limit : Optional [ int ] = None , last_evaluated_key : Optional [ Dict [ str , Dict [ str , Any ]]] = None , attributes_to_get : Optional [ Iterable [ str ]] = None , page_size : Optional [ int ] = None , rate_limit : Optional [ float ] = None , settings : OperationSettings = OperationSettings . default , ) -> pynamodb . models . ResultIterator [ _T ]: # \"\"\" Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb \"\"\" # CBC TODO support optional filter condition if supplied range_condition operand is \"=\" if ( not cache_store . cache_enabled ()) and ( filter_condition is not None ): log . warning ( \"Not using the cache\" ) return super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ) log . info ( 'Try the local_cache first' ) if isinstance ( filter_condition , Condition ): conn = cache_store . get_connection ( model_class = cls ) cached_rows = list ( cache_store . get_model ( conn , cls , range_key_condition , filter_condition )) # type: ignore minimum_expected_hits = cache_store . count_permutations ( filter_condition ) log . info ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) if len ( cached_rows ) >= minimum_expected_hits : return cached_rows # type: ignore if len ( cached_rows ) < minimum_expected_hits : log . warn ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) result = [] for res in super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ): cache_store . put_model ( conn , res ) result . append ( res ) return result # type: ignore @classmethod def create_table ( cls : Type [ _T ], wait : bool = False , read_capacity_units : Optional [ int ] = None , write_capacity_units : Optional [ int ] = None , billing_mode : Optional [ str ] = None , ignore_update_ttl_errors : bool = False , ): \"\"\" extends create_table to manage the local_cache table. \"\"\" if cache_store . cache_enabled (): log . info ( \"setup local cache\" ) conn = cache_store . get_connection ( model_class = cls ) cache_store . ensure_table_exists ( conn , model_class = cls ) return super () . create_table ( # type: ignore wait , read_capacity_units , write_capacity_units , billing_mode , ignore_update_ttl_errors , ) @classmethod def delete_table ( cls : Type [ _T ]): \"\"\" extends delete_table to manage the local_cache table. \"\"\" log . info ( 'drop the table ' ) return super () . delete_table () # type: ignore","title":"ModelCacheMixin"},{"location":"api/#toshi_hazard_store.model.caching.model_cache_mixin.ModelCacheMixin.create_table","text":"extends create_table to manage the local_cache table. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @classmethod def create_table ( cls : Type [ _T ], wait : bool = False , read_capacity_units : Optional [ int ] = None , write_capacity_units : Optional [ int ] = None , billing_mode : Optional [ str ] = None , ignore_update_ttl_errors : bool = False , ): \"\"\" extends create_table to manage the local_cache table. \"\"\" if cache_store . cache_enabled (): log . info ( \"setup local cache\" ) conn = cache_store . get_connection ( model_class = cls ) cache_store . ensure_table_exists ( conn , model_class = cls ) return super () . create_table ( # type: ignore wait , read_capacity_units , write_capacity_units , billing_mode , ignore_update_ttl_errors , )","title":"create_table()"},{"location":"api/#toshi_hazard_store.model.caching.model_cache_mixin.ModelCacheMixin.delete_table","text":"extends delete_table to manage the local_cache table. Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 116 117 118 119 120 121 122 @classmethod def delete_table ( cls : Type [ _T ]): \"\"\" extends delete_table to manage the local_cache table. \"\"\" log . info ( 'drop the table ' ) return super () . delete_table () # type: ignore","title":"delete_table()"},{"location":"api/#toshi_hazard_store.model.caching.model_cache_mixin.ModelCacheMixin.query","text":"Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb Source code in toshi_hazard_store/model/caching/model_cache_mixin.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @classmethod def query ( # type: ignore cls : Type [ _T ], hash_key : _KeyType , range_key_condition : Optional [ Condition ] = None , filter_condition : Optional [ Condition ] = None , consistent_read : bool = False , index_name : Optional [ str ] = None , scan_index_forward : Optional [ bool ] = None , limit : Optional [ int ] = None , last_evaluated_key : Optional [ Dict [ str , Dict [ str , Any ]]] = None , attributes_to_get : Optional [ Iterable [ str ]] = None , page_size : Optional [ int ] = None , rate_limit : Optional [ float ] = None , settings : OperationSettings = OperationSettings . default , ) -> pynamodb . models . ResultIterator [ _T ]: # \"\"\" Proxy query function which trys to use the local_cache before hitting AWS via Pynamodb \"\"\" # CBC TODO support optional filter condition if supplied range_condition operand is \"=\" if ( not cache_store . cache_enabled ()) and ( filter_condition is not None ): log . warning ( \"Not using the cache\" ) return super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ) log . info ( 'Try the local_cache first' ) if isinstance ( filter_condition , Condition ): conn = cache_store . get_connection ( model_class = cls ) cached_rows = list ( cache_store . get_model ( conn , cls , range_key_condition , filter_condition )) # type: ignore minimum_expected_hits = cache_store . count_permutations ( filter_condition ) log . info ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) if len ( cached_rows ) >= minimum_expected_hits : return cached_rows # type: ignore if len ( cached_rows ) < minimum_expected_hits : log . warn ( 'permutations: %s cached_rows: %s ' % ( minimum_expected_hits , len ( cached_rows ))) result = [] for res in super () . query ( # type: ignore hash_key , range_key_condition , filter_condition , consistent_read , index_name , scan_index_forward , limit , last_evaluated_key , attributes_to_get , page_size , rate_limit , settings , ): cache_store . put_model ( conn , res ) result . append ( res ) return result # type: ignore","title":"query()"},{"location":"api/#toshi_hazard_store.model.constraints","text":"","title":"constraints"},{"location":"api/#toshi_hazard_store.model.constraints.AggregationEnum","text":"Bases: Enum Defines the values available for aggregations. Source code in toshi_hazard_store/model/constraints.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class AggregationEnum ( Enum ): \"\"\"Defines the values available for aggregations.\"\"\" MEAN = 'mean' COV = 'cov' STD = 'std' _005 = '0.005' _01 = '0.01' _025 = '0.025' _05 = '0.05' _10 = '0.1' _20 = '0.2' _30 = '0.3' _40 = '0.4' _50 = '0.5' _60 = '0.6' _70 = '0.7' _80 = '0.8' _90 = '0.9' _95 = '0.95' _975 = '0.975' _99 = '0.99' _995 = '0.995'","title":"AggregationEnum"},{"location":"api/#toshi_hazard_store.model.constraints.IntensityMeasureTypeEnum","text":"Bases: Enum Defines the values available for IMTs. Source code in toshi_hazard_store/model/constraints.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class IntensityMeasureTypeEnum ( Enum ): \"\"\" Defines the values available for IMTs. \"\"\" PGA = 'PGA' SA_0_1 = 'SA(0.1)' SA_0_15 = 'SA(0.15)' SA_0_2 = 'SA(0.2)' SA_0_25 = 'SA(0.25)' SA_0_3 = 'SA(0.3)' SA_0_35 = 'SA(0.35)' SA_0_4 = 'SA(0.4)' SA_0_5 = 'SA(0.5)' SA_0_6 = 'SA(0.6)' SA_0_7 = 'SA(0.7)' SA_0_8 = 'SA(0.8)' SA_0_9 = 'SA(0.9)' SA_1_0 = 'SA(1.0)' SA_1_25 = 'SA(1.25)' SA_1_5 = 'SA(1.5)' SA_1_75 = 'SA(1.75)' SA_2_0 = 'SA(2.0)' SA_2_5 = 'SA(2.5)' SA_3_0 = 'SA(3.0)' SA_3_5 = 'SA(3.5)' SA_4_0 = 'SA(4.0)' SA_4_5 = 'SA(4.5)' SA_5_0 = 'SA(5.0)' SA_6_0 = 'SA(6.0)' SA_7_5 = 'SA(7.5)' SA_10_0 = 'SA(10.0)'","title":"IntensityMeasureTypeEnum"},{"location":"api/#toshi_hazard_store.model.constraints.ProbabilityEnum","text":"Bases: Enum Defines the values available for probabilities. store values as float representing probability in 1 year Source code in toshi_hazard_store/model/constraints.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class ProbabilityEnum ( Enum ): \"\"\" Defines the values available for probabilities. store values as float representing probability in 1 year \"\"\" _86_PCT_IN_50YRS = 3.8559e-02 _63_PCT_IN_50YRS = 1.9689e-02 _39_PCT_IN_50YRS = 9.8372e-03 _18_PCT_IN_50YRS = 3.9612e-03 _10_PCT_IN_50YRS = 2.1050e-03 _5_PCT_IN_50YRS = 1.0253e-03 _2_PCT_IN_50YRS = 4.0397e-04 _1_PCT_IN_50YRS = 2.0099e-04","title":"ProbabilityEnum"},{"location":"api/#toshi_hazard_store.model.constraints.VS30Enum","text":"Bases: Enum Defines the values available for VS30. Source code in toshi_hazard_store/model/constraints.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class VS30Enum ( Enum ): \"\"\" Defines the values available for VS30. \"\"\" _0 = 0 # indicates that this value is not used _150 = 150 _175 = 175 _200 = 200 _225 = 225 _250 = 250 _275 = 275 _300 = 300 _350 = 350 _375 = 375 _400 = 400 _450 = 450 _500 = 500 _525 = 525 _550 = 550 _600 = 600 _650 = 650 _700 = 700 _750 = 750 _800 = 800 _850 = 850 _900 = 900 _950 = 950 _1000 = 1000 _1050 = 1050 _1100 = 1100 _1500 = 1500","title":"VS30Enum"},{"location":"api/#toshi_hazard_store.model.disagg_models","text":"This module defines the pynamodb tables used to store openquake data. Third iteration","title":"disagg_models"},{"location":"api/#toshi_hazard_store.model.disagg_models.DisaggAggregationBase","text":"Bases: LocationIndexedModel Store aggregated disaggregations. Source code in toshi_hazard_store/model/disagg_models.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class DisaggAggregationBase ( LocationIndexedModel ): \"\"\"Store aggregated disaggregations.\"\"\" hazard_model_id = UnicodeAttribute () imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) hazard_agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) # eg MEAN disagg_agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) disaggs = CompressedPickleAttribute () # a very compressible numpy array, bins = PickleAttribute () # a much smaller numpy array shaking_level = FloatAttribute () probability = EnumAttribute ( ProbabilityEnum ) # eg TEN_PCT_IN_50YRS def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = ( f ' { self . hazard_model_id } : { self . hazard_agg } : { self . disagg_agg } :' f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . probability . name } ' ) return self","title":"DisaggAggregationBase"},{"location":"api/#toshi_hazard_store.model.disagg_models.DisaggAggregationBase.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/disagg_models.py 41 42 43 44 45 46 47 48 49 50 51 52 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = ( f ' { self . hazard_model_id } : { self . hazard_agg } : { self . disagg_agg } :' f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . probability . name } ' ) return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.disagg_models.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/disagg_models.py 116 117 118 119 120 121 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.disagg_models.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/disagg_models.py 107 108 109 110 111 112 113 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.gridded_hazard","text":"This module defines the pynamodb tables used to store THH.","title":"gridded_hazard"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.GriddedHazard","text":"Bases: Model Grid points defined in location_grid_id has a values in grid_poes. Source code in toshi_hazard_store/model/gridded_hazard.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class GriddedHazard ( Model ): \"\"\"Grid points defined in location_grid_id has a values in grid_poes.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) sort_key = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_model_id = UnicodeAttribute () location_grid_id = UnicodeAttribute () vs30 = EnumConstrainedIntegerAttribute ( VS30Enum ) imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) poe = FloatAttribute () grid_poes = CompressedListAttribute () @staticmethod def new_model ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe , grid_poes ) -> 'GriddedHazard' : obj = GriddedHazard ( hazard_model_id = hazard_model_id , location_grid_id = location_grid_id , vs30 = vs30 , imt = imt , agg = agg , poe = poe , grid_poes = grid_poes , ) obj . partition_key = f \" { obj . hazard_model_id } \" obj . sort_key = f \" { obj . hazard_model_id } : { obj . location_grid_id } : { obj . vs30 } : { obj . imt } : { obj . agg } : { obj . poe } \" return obj","title":"GriddedHazard"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.GriddedHazard.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/gridded_hazard.py 26 27 28 29 30 31 32 33 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/gridded_hazard.py 79 80 81 82 83 84 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/gridded_hazard.py 70 71 72 73 74 75 76 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.location_indexed_model","text":"","title":"location_indexed_model"},{"location":"api/#toshi_hazard_store.model.location_indexed_model.LocationIndexedModel","text":"Bases: Model Model base class. Source code in toshi_hazard_store/model/location_indexed_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = EnumConstrainedIntegerAttribute ( VS30Enum ) site_vs30 = FloatAttribute ( null = True ) created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"LocationIndexedModel"},{"location":"api/#toshi_hazard_store.model.location_indexed_model.LocationIndexedModel.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/location_indexed_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_models","text":"This module defines the pynamodb tables used to store openquake data. Third iteration","title":"openquake_models"},{"location":"api/#toshi_hazard_store.model.openquake_models.HazardAggregation","text":"Bases: ModelCacheMixin , LocationIndexedModel A pynamodb model for aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_models.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class HazardAggregation ( ModelCacheMixin , LocationIndexedModel ): \"\"\"A pynamodb model for aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = EnumConstrainedUnicodeAttribute ( IntensityMeasureTypeEnum ) agg = EnumConstrainedUnicodeAttribute ( AggregationEnum ) values = ListAttribute ( of = LevelValuePairAttribute ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1","title":"HazardAggregation"},{"location":"api/#toshi_hazard_store.model.openquake_models.HazardAggregation.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 92 93 94 95 96 97 98 99 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_models.HazardAggregation.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_models.py 107 108 109 110 111 112 113 114 115 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_models.HazardAggregation.to_csv","text":"Generate lists ready for csv module - including a header, followed by n rows. Source code in toshi_hazard_store/model/openquake_models.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1","title":"to_csv()"},{"location":"api/#toshi_hazard_store.model.openquake_models.OpenquakeRealization","text":"Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_models.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"OpenquakeRealization"},{"location":"api/#toshi_hazard_store.model.openquake_models.OpenquakeRealization.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 150 151 152 153 154 155 156 157 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_models.OpenquakeRealization.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_models.py 170 171 172 173 174 175 176 177 178 179 180 181 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_models.ToshiOpenquakeMeta","text":"Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_models.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON","title":"ToshiOpenquakeMeta"},{"location":"api/#toshi_hazard_store.model.openquake_models.ToshiOpenquakeMeta.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_models.py 32 33 34 35 36 37 38 39 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_models.vs30_nloc001_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_models.py 76 77 78 79 80 81 82 83 84 85 86 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc001_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_models.vs30_nloc1_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_models.py 63 64 65 66 67 68 69 70 71 72 73 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc1_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_models.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_models.py 199 200 201 202 203 204 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.openquake_models.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_models.py 191 192 193 194 195 196 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.multi_batch","text":"","title":"multi_batch"},{"location":"api/#toshi_hazard_store.multi_batch.DynamoBatchWorker","text":"Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") # if self.model == model.ToshiOpenquakeHazardCurveStatsV2: # query.batch_save_hcurve_stats_v2(self.toshi_id, models=models) # elif self.model == model.ToshiOpenquakeHazardCurveRlzsV2: # query.batch_save_hcurve_rlzs_v2(self.toshi_id, models=models) if self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" )","title":"DynamoBatchWorker"},{"location":"api/#toshi_hazard_store.oq_import","text":"","title":"oq_import"},{"location":"api/#toshi_hazard_store.oq_import.export_v3","text":"","title":"export_v3"},{"location":"api/#toshi_hazard_store.oq_import.export_v3.export_meta_v3","text":"Extract and same the meta data. Source code in toshi_hazard_store/oq_import/export_v3.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) vs30 = oq . reference_vs30_value if math . isnan ( vs30 ): vs30 = 0 obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( vs30 )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( vs30 ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj )","title":"export_meta_v3()"},{"location":"api/#toshi_hazard_store.pynamodb_settings","text":"pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered.","title":"pynamodb_settings"},{"location":"api/#toshi_hazard_store.query","text":"","title":"query"},{"location":"api/#toshi_hazard_store.query.disagg_queries","text":"Queries for saving and retrieving gridded hazard convenience.","title":"disagg_queries"},{"location":"api/#toshi_hazard_store.query.disagg_queries.get_one_disagg_aggregation","text":"Fetch model based on single model arguments. Source code in toshi_hazard_store/query/disagg_queries.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def get_one_disagg_aggregation ( hazard_model_id : str , hazard_agg : AggregationEnum , disagg_agg : AggregationEnum , location : CodedLocation , vs30 : float , imt : str , poe : ProbabilityEnum , model : Type [ Union [ mDAE , mDAO ]] = mDAE , ) -> Union [ mDAE , mDAO , None ]: \"\"\"Fetch model based on single model arguments.\"\"\" qry = model . query ( downsample_code ( location , 0.1 ), range_key_condition = model . sort_key == f ' { hazard_model_id } : { hazard_agg . value } : { disagg_agg . value } :' f ' { location } : { vs30 } : { imt } : { poe . name } ' , # type: ignore ) log . debug ( f \"get_one_disagg_aggregation: qry { qry } \" ) result : List [ Union [ mDAE , mDAO ]] = list ( qry ) assert len ( result ) in [ 0 , 1 ] if len ( result ): return result [ 0 ] return None","title":"get_one_disagg_aggregation()"},{"location":"api/#toshi_hazard_store.query.gridded_hazard_query","text":"Queries for saving and retrieving gridded hazard convenience.","title":"gridded_hazard_query"},{"location":"api/#toshi_hazard_store.query.gridded_hazard_query.get_gridded_hazard","text":"Fetch GriddedHazard based on criteria. Source code in toshi_hazard_store/query/gridded_hazard_query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_gridded_hazard ( hazard_model_ids : Iterable [ str ], location_grid_ids : Iterable [ str ], vs30s : Iterable [ float ], imts : Iterable [ str ], aggs : Iterable [ str ], poes : Iterable [ float ], ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on criteria.\"\"\" # partition_key = f\"{obj.hazard_model_id}\" # sort_key = f\"{obj.hazard_model_id}:{obj.location_grid_id}:{obj.vs30}:{obj.imt}:{obj.agg}:{obj.poe}\" def build_sort_key ( hazard_model_id , grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build sort_key.\"\"\" sort_key = hazard_model_id sort_key = sort_key + f \": { sorted ( grid_ids )[ 0 ] } \" if grid_ids else sort_key sort_key = sort_key + f \": { sorted ( vs30s )[ 0 ] } \" if grid_ids and vs30s else sort_key sort_key = sort_key + f \": { sorted ( imts )[ 0 ] } \" if grid_ids and vs30s and imts else sort_key sort_key = sort_key + f \": { sorted ( aggs )[ 0 ] } \" if grid_ids and vs30s and imts and aggs else sort_key sort_key = sort_key + f \": { sorted ( poes )[ 0 ] } \" if grid_ids and vs30s and imts and aggs and poes else sort_key return sort_key def build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build filter condition.\"\"\" condition_expr = mGH . hazard_model_id == hazard_model_id if location_grid_ids : condition_expr = condition_expr & mGH . location_grid_id . is_in ( * location_grid_ids ) if vs30s : condition_expr = condition_expr & mGH . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mGH . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mGH . agg . is_in ( * aggs ) if poes : condition_expr = condition_expr & mGH . poe . is_in ( * poes ) return condition_expr # TODO: this can be parallelised/optimised. for hazard_model_id in hazard_model_ids : sort_key_first_val = build_sort_key ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) condition_expr = build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) log . debug ( f 'sort_key_first_val { sort_key_first_val } ' ) log . debug ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mGH . query ( hazard_model_id , mGH . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mGH . query ( hazard_model_id , mGH . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_gridded_hazard()"},{"location":"api/#toshi_hazard_store.query.gridded_hazard_query.get_one_gridded_hazard","text":"Fetch GriddedHazard based on single criteria. Source code in toshi_hazard_store/query/gridded_hazard_query.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_one_gridded_hazard ( hazard_model_id : str , location_grid_id : str , vs30 : float , imt : str , agg : str , poe : float , ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on single criteria.\"\"\" qry = mGH . query ( hazard_model_id , mGH . sort_key == f ' { hazard_model_id } : { location_grid_id } : { vs30 } : { imt } : { agg } : { poe } ' ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_one_gridded_hazard()"},{"location":"api/#toshi_hazard_store.query.hazard_query","text":"Queries for saving and retrieving openquake hazard results with convenience.","title":"hazard_query"},{"location":"api/#toshi_hazard_store.query.hazard_query.first_vs30_key","text":"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. Source code in toshi_hazard_store/query/hazard_query.py 61 62 63 64 65 66 67 68 69 70 71 def first_vs30_key ( vs30s ): \"\"\"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. \"\"\" if have_mixed_length_vs30s ( vs30s ): vsLong = filter ( lambda x : x >= 1000 , vs30s ) return str ( int ( min ( vsLong ) / 10 )) . zfill ( model . VS30_KEYLEN ) return str ( min ( vs30s )) . zfill ( model . VS30_KEYLEN )","title":"first_vs30_key()"},{"location":"api/#toshi_hazard_store.query.hazard_query.get_hazard_curves","text":"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query/hazard_query.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], local_cache : bool = False , ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): log . info ( 'hash_key %s ' % hash_location_code ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) log . debug ( 'sort_key_first_val: %s ' % sort_key_first_val ) log . debug ( 'condition_expr: %s ' % condition_expr ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( \"get_hazard_rlz_curves_v3: qry %s \" % qry ) hits = 0 for hit in qry : hits += 1 yield ( hit ) log . info ( 'hash_key %s has %s hits' % ( hash_location_code , hits ))","title":"get_hazard_curves()"},{"location":"api/#toshi_hazard_store.query.hazard_query.get_hazard_metadata_v3","text":"Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query/hazard_query.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit )","title":"get_hazard_metadata_v3()"},{"location":"api/#toshi_hazard_store.query.hazard_query.get_rlz_curves_v3","text":"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query/hazard_query.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit )","title":"get_rlz_curves_v3()"},{"location":"api/#toshi_hazard_store.query.hazard_query.have_mixed_length_vs30s","text":"Does the list of vs30s require mixed length index keys. Source code in toshi_hazard_store/query/hazard_query.py 50 51 52 53 54 55 56 57 58 def have_mixed_length_vs30s ( vs30s ): \"\"\"Does the list of vs30s require mixed length index keys.\"\"\" max_vs30 = max ( vs30s ) min_vs30 = min ( vs30s ) if max_vs30 >= 1000 : if min_vs30 < 1000 : return True else : return False","title":"have_mixed_length_vs30s()"},{"location":"api/#toshi_hazard_store.transform","text":"Helper functions to export an openquake calculation and save it with toshi-hazard-store.","title":"transform"},{"location":"api/#toshi_hazard_store.transform.parse_logic_tree_branches","text":"Extract the dataframes. Source code in toshi_hazard_store/transform.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt","title":"parse_logic_tree_branches()"},{"location":"api/#toshi_hazard_store.utils","text":"Common utilities.","title":"utils"},{"location":"api/#toshi_hazard_store.utils.normalise_site_code","text":"Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"normalise_site_code()"},{"location":"changelog/","text":"Changelog \u00b6 [0.7.0] - 2023-04-17 \u00b6 Changed \u00b6 update openquake dependency 3.16 update nzshm-common dependency 0.5.0 fix SA(0.7) value Added \u00b6 script ths_cache to prepopulate and test caching local caching feature more spectral periods in constraint enum new constraints to existing THS models fix enum validations and apply to model fields Removed \u00b6 remove v2 type options from batch save [0.6.0] - 2023-02-15 \u00b6 Changed \u00b6 refactor model package refactor model.attributes package more test coverage Added \u00b6 two new models for DisaggAggregations validation via Enum for aggregation values new enumerations and constraints for probabilities, IMTS and VS30s [0.5.5] - 2022-10-06 \u00b6 Changed \u00b6 fix for queries with mixed length vs30 index keys migrate more print statements to logging.debug [0.5.4] - 2022-09-27 \u00b6 Added \u00b6 new query get_one_gridded_hazard -m option to script to export meta tables only Changed \u00b6 migrated print statements to logging.debug removed monkey patch for BASE183 - it iss in oqengine 3.15 now more test cover [0.5.3] - 2022-08-18 \u00b6 Changed \u00b6 using nzshm-common==0.3.2 from pypi. specify poetry==1.2.0b3 in all the GHA yml files. [0.5.1] - 2022-08-17 \u00b6 Added \u00b6 THS_HazardAggregation table support for csv serialisation. Changed \u00b6 refactoring/renaming openquake import modules. Removed \u00b6 one openquake test no longer works as expected. It's off-piste so skipping it for now. data_functions migrated to THP branch_combinator migrated to THP [0.5.0] - 2022-08-03 \u00b6 Added \u00b6 V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling. Removed \u00b6 realisation aggregration computations. These have moving to toshi-hazard-post [0.4.1] - 2022-06-22 \u00b6 Added \u00b6 multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types Changed \u00b6 V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules. [0.4.0] - 2022-06-10 \u00b6 Added \u00b6 new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency. Changed \u00b6 meta dataframes are cut back to dstore defaults to minimise size. [0.3.2] - 2022-05-30 \u00b6 Added \u00b6 meta.aggs attribute meta.inv_tme attribute Changed \u00b6 store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks). [0.3.1] - 2022-05-29 \u00b6 Changed \u00b6 updated usage. [0.3.0] - 2022-05-28 \u00b6 Added \u00b6 store_hazard script for openquake systems. Changed \u00b6 tightened up model attributes names. [0.2.0] - 2022-05-27 \u00b6 Added \u00b6 query api improvements added meta table new query methods for meta and rlzs Changed \u00b6 moved vs30 from curves to meta updated docs [0.1.3] - 2022-05-26 \u00b6 Changed \u00b6 fixed mkdoc rendering of python & markdown. [0.1.2] - 2022-05-26 \u00b6 Changed \u00b6 fix poetry lockfile [0.1.1] - 2022-05-26 \u00b6 Added \u00b6 First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#070---2023-04-17","text":"","title":"[0.7.0] - 2023-04-17"},{"location":"changelog/#changed","text":"update openquake dependency 3.16 update nzshm-common dependency 0.5.0 fix SA(0.7) value","title":"Changed"},{"location":"changelog/#added","text":"script ths_cache to prepopulate and test caching local caching feature more spectral periods in constraint enum new constraints to existing THS models fix enum validations and apply to model fields","title":"Added"},{"location":"changelog/#removed","text":"remove v2 type options from batch save","title":"Removed"},{"location":"changelog/#060---2023-02-15","text":"","title":"[0.6.0] - 2023-02-15"},{"location":"changelog/#changed_1","text":"refactor model package refactor model.attributes package more test coverage","title":"Changed"},{"location":"changelog/#added_1","text":"two new models for DisaggAggregations validation via Enum for aggregation values new enumerations and constraints for probabilities, IMTS and VS30s","title":"Added"},{"location":"changelog/#055---2022-10-06","text":"","title":"[0.5.5] - 2022-10-06"},{"location":"changelog/#changed_2","text":"fix for queries with mixed length vs30 index keys migrate more print statements to logging.debug","title":"Changed"},{"location":"changelog/#054---2022-09-27","text":"","title":"[0.5.4] - 2022-09-27"},{"location":"changelog/#added_2","text":"new query get_one_gridded_hazard -m option to script to export meta tables only","title":"Added"},{"location":"changelog/#changed_3","text":"migrated print statements to logging.debug removed monkey patch for BASE183 - it iss in oqengine 3.15 now more test cover","title":"Changed"},{"location":"changelog/#053---2022-08-18","text":"","title":"[0.5.3] - 2022-08-18"},{"location":"changelog/#changed_4","text":"using nzshm-common==0.3.2 from pypi. specify poetry==1.2.0b3 in all the GHA yml files.","title":"Changed"},{"location":"changelog/#051---2022-08-17","text":"","title":"[0.5.1] - 2022-08-17"},{"location":"changelog/#added_3","text":"THS_HazardAggregation table support for csv serialisation.","title":"Added"},{"location":"changelog/#changed_5","text":"refactoring/renaming openquake import modules.","title":"Changed"},{"location":"changelog/#removed_1","text":"one openquake test no longer works as expected. It's off-piste so skipping it for now. data_functions migrated to THP branch_combinator migrated to THP","title":"Removed"},{"location":"changelog/#050---2022-08-03","text":"","title":"[0.5.0] - 2022-08-03"},{"location":"changelog/#added_4","text":"V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling.","title":"Added"},{"location":"changelog/#removed_2","text":"realisation aggregration computations. These have moving to toshi-hazard-post","title":"Removed"},{"location":"changelog/#041---2022-06-22","text":"","title":"[0.4.1] - 2022-06-22"},{"location":"changelog/#added_5","text":"multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types","title":"Added"},{"location":"changelog/#changed_6","text":"V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules.","title":"Changed"},{"location":"changelog/#040---2022-06-10","text":"","title":"[0.4.0] - 2022-06-10"},{"location":"changelog/#added_6","text":"new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency.","title":"Added"},{"location":"changelog/#changed_7","text":"meta dataframes are cut back to dstore defaults to minimise size.","title":"Changed"},{"location":"changelog/#032---2022-05-30","text":"","title":"[0.3.2] - 2022-05-30"},{"location":"changelog/#added_7","text":"meta.aggs attribute meta.inv_tme attribute","title":"Added"},{"location":"changelog/#changed_8","text":"store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks).","title":"Changed"},{"location":"changelog/#031---2022-05-29","text":"","title":"[0.3.1] - 2022-05-29"},{"location":"changelog/#changed_9","text":"updated usage.","title":"Changed"},{"location":"changelog/#030---2022-05-28","text":"","title":"[0.3.0] - 2022-05-28"},{"location":"changelog/#added_8","text":"store_hazard script for openquake systems.","title":"Added"},{"location":"changelog/#changed_10","text":"tightened up model attributes names.","title":"Changed"},{"location":"changelog/#020---2022-05-27","text":"","title":"[0.2.0] - 2022-05-27"},{"location":"changelog/#added_9","text":"query api improvements added meta table new query methods for meta and rlzs","title":"Added"},{"location":"changelog/#changed_11","text":"moved vs30 from curves to meta updated docs","title":"Changed"},{"location":"changelog/#013---2022-05-26","text":"","title":"[0.1.3] - 2022-05-26"},{"location":"changelog/#changed_12","text":"fixed mkdoc rendering of python & markdown.","title":"Changed"},{"location":"changelog/#012---2022-05-26","text":"","title":"[0.1.2] - 2022-05-26"},{"location":"changelog/#changed_13","text":"fix poetry lockfile","title":"Changed"},{"location":"changelog/#011---2022-05-26","text":"","title":"[0.1.1] - 2022-05-26"},{"location":"changelog/#added_10","text":"First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 Environment & Authorisation pre-requisites \u00b6 NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication) toshi-hazard-store (library) \u00b6 To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break store_hazard (script) \u00b6 TODO decribe usage of the upload script","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#environment--authorisation-pre-requisites","text":"NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication)","title":"Environment &amp; Authorisation pre-requisites"},{"location":"usage/#toshi-hazard-store-library","text":"To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break","title":"toshi-hazard-store (library)"},{"location":"usage/#store_hazard-script","text":"TODO decribe usage of the upload script","title":"store_hazard (script)"}]}