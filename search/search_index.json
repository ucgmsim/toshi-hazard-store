{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"toshi-hazard-store \u00b6 Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only Features \u00b6 Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#toshi-hazard-store","text":"Documentation: https://GNS-Science.github.io/toshi-hazard-store GitHub: https://github.com/GNS-Science/toshi-hazard-store PyPI: https://pypi.org/project/toshi-hazard-store/ Free software: GPL-3.0-only","title":"toshi-hazard-store"},{"location":"#features","text":"Main purpose is to upload Openquake hazard results to a DynamodDB tables defined herein. relates the results to the toshi hazard id identifying the OQ hazard job run. extracts metadata from the openquake hdf5 solution","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for toshi-hazard-store. config \u00b6 This module exports comfiguration for the current system. boolean_env ( environ_name , default = 'FALSE' ) \u00b6 Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ]) gridded_hazard_query \u00b6 Queries for saving and retrieving gridded hazard convenience. get_gridded_hazard ( hazard_model_ids , location_grid_ids , vs30s , imts , aggs , poes ) \u00b6 Fetch GriddedHazard based on criteria. Source code in toshi_hazard_store/gridded_hazard_query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_gridded_hazard ( hazard_model_ids : Iterable [ str ], location_grid_ids : Iterable [ str ], vs30s : Iterable [ float ], imts : Iterable [ str ], aggs : Iterable [ str ], poes : Iterable [ float ], ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on criteria.\"\"\" # partition_key = f\"{obj.hazard_model_id}\" # sort_key = f\"{obj.hazard_model_id}:{obj.location_grid_id}:{obj.vs30}:{obj.imt}:{obj.agg}:{obj.poe}\" def build_sort_key ( hazard_model_id , grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build sort_key.\"\"\" sort_key = hazard_model_id sort_key = sort_key + f \": { sorted ( grid_ids )[ 0 ] } \" if grid_ids else sort_key sort_key = sort_key + f \": { sorted ( vs30s )[ 0 ] } \" if grid_ids and vs30s else sort_key sort_key = sort_key + f \": { sorted ( imts )[ 0 ] } \" if grid_ids and vs30s and imts else sort_key sort_key = sort_key + f \": { sorted ( aggs )[ 0 ] } \" if grid_ids and vs30s and imts and aggs else sort_key sort_key = sort_key + f \": { sorted ( poes )[ 0 ] } \" if grid_ids and vs30s and imts and aggs and poes else sort_key return sort_key def build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build filter condition.\"\"\" condition_expr = mGH . hazard_model_id == hazard_model_id if location_grid_ids : condition_expr = condition_expr & mGH . location_grid_id . is_in ( * location_grid_ids ) if vs30s : condition_expr = condition_expr & mGH . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mGH . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mGH . agg . is_in ( * aggs ) if poes : condition_expr = condition_expr & mGH . poe . is_in ( * poes ) return condition_expr # TODO: this can be parallelised/optimised. for hazard_model_id in hazard_model_ids : sort_key_first_val = build_sort_key ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) condition_expr = build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) log . debug ( f 'sort_key_first_val { sort_key_first_val } ' ) log . debug ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mGH . query ( hazard_model_id , mGH . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mGH . query ( hazard_model_id , mGH . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit ) get_one_gridded_hazard ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe ) \u00b6 Fetch GriddedHazard based on single criteria. Source code in toshi_hazard_store/gridded_hazard_query.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_one_gridded_hazard ( hazard_model_id : str , location_grid_id : str , vs30 : float , imt : str , agg : str , poe : float , ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on single criteria.\"\"\" qry = mGH . query ( hazard_model_id , mGH . sort_key == f ' { hazard_model_id } : { location_grid_id } : { vs30 } : { imt } : { agg } : { poe } ' ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit ) model \u00b6 drop_tables () \u00b6 Drop em Source code in toshi_hazard_store/model/__init__.py 19 20 21 22 def drop_tables (): \"\"\"Drop em\"\"\" drop_tables_v3 () drop_gridded () migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 13 14 15 16 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_v3 () migrate_gridded () gridded_hazard \u00b6 This module defines the pynamodb tables used to store THH. CompressedJsonicAttribute \u00b6 Bases: Attribute A compressed, json serialisable model attribute Source code in toshi_hazard_store/model/gridded_hazard.py 24 25 26 27 28 29 30 31 32 33 34 35 class CompressedJsonicAttribute ( Attribute ): \"\"\" A compressed, json serialisable model attribute \"\"\" attr_type = STRING def serialize ( self , value : Any ) -> str : return compress_string ( json . dumps ( value )) # could this be pickle?? def deserialize ( self , value : str ) -> Union [ Dict , List ]: return json . loads ( decompress_string ( value )) CompressedListAttribute \u00b6 Bases: CompressedJsonicAttribute A compressed list of floats attribute. Source code in toshi_hazard_store/model/gridded_hazard.py 38 39 40 41 42 43 44 45 46 47 48 class CompressedListAttribute ( CompressedJsonicAttribute ): \"\"\" A compressed list of floats attribute. \"\"\" def serialize ( self , value : List [ float ]) -> str : if value is not None and not isinstance ( value , list ): raise TypeError ( f \"value has invalid type ' { type ( value ) } '; List[float])expected\" , ) return super () . serialize ( value ) GriddedHazard \u00b6 Bases: Model Grid points defined in location_grid_id has a values in grid_poes. Source code in toshi_hazard_store/model/gridded_hazard.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class GriddedHazard ( Model ): \"\"\"Grid points defined in location_grid_id has a values in grid_poes.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) sort_key = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_model_id = UnicodeAttribute () location_grid_id = UnicodeAttribute () vs30 = FloatAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () poe = FloatAttribute () grid_poes = CompressedListAttribute () @staticmethod def new_model ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe , grid_poes ) -> 'GriddedHazard' : obj = GriddedHazard ( hazard_model_id = hazard_model_id , location_grid_id = location_grid_id , vs30 = vs30 , imt = imt , agg = agg , poe = poe , grid_poes = grid_poes , ) obj . partition_key = f \" { obj . hazard_model_id } \" obj . sort_key = f \" { obj . hazard_model_id } : { obj . location_grid_id } : { obj . vs30 } : { obj . imt } : { obj . agg } : { obj . poe } \" return obj Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/gridded_hazard.py 54 55 56 57 58 59 60 61 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/gridded_hazard.py 107 108 109 110 111 112 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/gridded_hazard.py 98 99 100 101 102 103 104 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) openquake_v1_model \u00b6 This module defines the pynamodb tables used to store openquake data. LevelValuePairAttribute \u00b6 Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/openquake_v1_model.py 6 7 8 9 10 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False ) openquake_v2_model \u00b6 This module defines the pynamodb tables used to store openquake v2 data. IMTValuesAttribute \u00b6 Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/openquake_v2_model.py 6 7 8 9 10 11 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute ) openquake_v3_model \u00b6 This module defines the pynamodb tables used to store openquake data. Third iteration HazardAggregation \u00b6 Bases: LocationIndexedModel Stores aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class HazardAggregation ( LocationIndexedModel ): \"\"\"Stores aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1 Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 133 134 135 136 137 138 139 140 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 148 149 150 151 152 153 154 155 156 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self to_csv ( models ) staticmethod \u00b6 Generate lists ready for csv module - including a header, followed by n rows. Source code in toshi_hazard_store/model/openquake_v3_model.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1 LocationIndexedModel \u00b6 Bases: Model Model base class. Source code in toshi_hazard_store/model/openquake_v3_model.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self OpenquakeRealization \u00b6 Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 191 192 193 194 195 196 197 198 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover set_location ( location ) \u00b6 Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 211 212 213 214 215 216 217 218 219 220 221 222 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self ToshiOpenquakeMeta \u00b6 Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v3_model.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON Meta \u00b6 DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 38 39 40 41 42 43 44 45 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover vs30_nloc001_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 82 83 84 85 86 87 88 89 90 91 92 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True ) vs30_nloc1_gt_rlz_index \u00b6 Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 69 70 71 72 73 74 75 76 77 78 79 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True ) drop_tables () \u00b6 Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v3_model.py 237 238 239 240 241 242 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' ) migrate () \u00b6 Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v3_model.py 228 229 230 231 232 233 234 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" ) multi_batch \u00b6 DynamoBatchWorker \u00b6 Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") if self . model == model . ToshiOpenquakeHazardCurveStatsV2 : query . batch_save_hcurve_stats_v2 ( self . toshi_id , models = models ) elif self . model == model . ToshiOpenquakeHazardCurveRlzsV2 : query . batch_save_hcurve_rlzs_v2 ( self . toshi_id , models = models ) elif self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" ) oq_import \u00b6 export_v3 \u00b6 export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ) \u00b6 Extract and same the meta data. Source code in toshi_hazard_store/oq_import/export_v3.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( oq . reference_vs30_value )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( oq . reference_vs30_value ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj ) pynamodb_settings \u00b6 pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered. query_v3 \u00b6 Queries for saving and retrieving openquake hazard results with convenience. first_vs30_key ( vs30s ) \u00b6 This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. Source code in toshi_hazard_store/query_v3.py 61 62 63 64 65 66 67 68 69 70 71 def first_vs30_key ( vs30s ): \"\"\"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. \"\"\" if have_mixed_length_vs30s ( vs30s ): vsLong = filter ( lambda x : x >= 1000 , vs30s ) return str ( int ( min ( vsLong ) / 10 )) . zfill ( model . VS30_KEYLEN ) return str ( min ( vs30s )) . zfill ( model . VS30_KEYLEN ) get_hazard_curves ( locs = [], vs30s = [], hazard_model_ids = [], imts = [], aggs = []) \u00b6 Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query_v3.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): log . debug ( 'hash_key %s ' % hash_location_code ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) log . debug ( 'sort_key_first_val: %s ' % sort_key_first_val ) log . debug ( 'condition_expr: %s ' % condition_expr ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( \"get_hazard_rlz_curves_v3: qry %s \" % qry ) for hit in qry : yield ( hit ) get_hazard_metadata_v3 ( haz_sol_ids = None , vs30_vals = None ) \u00b6 Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query_v3.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit ) get_rlz_curves_v3 ( locs = [], vs30s = [], rlzs = [], tids = [], imts = []) \u00b6 Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query_v3.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit ) have_mixed_length_vs30s ( vs30s ) \u00b6 Does the list of vs30s require mixed length index keys. Source code in toshi_hazard_store/query_v3.py 50 51 52 53 54 55 56 57 58 def have_mixed_length_vs30s ( vs30s ): \"\"\"Does the list of vs30s require mixed length index keys.\"\"\" max_vs30 = max ( vs30s ) min_vs30 = min ( vs30s ) if max_vs30 >= 1000 : if min_vs30 < 1000 : return True else : return False transform \u00b6 Helper functions to export an openquake calculation and save it with toshi-hazard-store. parse_logic_tree_branches ( file_id ) \u00b6 Extract the dataframes. Source code in toshi_hazard_store/transform.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt utils \u00b6 Common utilities. normalise_site_code ( oq_site_object , force_normalized = False ) \u00b6 Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"Modules"},{"location":"api/#toshi_hazard_store.config","text":"This module exports comfiguration for the current system.","title":"config"},{"location":"api/#toshi_hazard_store.config.boolean_env","text":"Helper function. Source code in toshi_hazard_store/config.py 6 7 8 def boolean_env ( environ_name : str , default : str = 'FALSE' ) -> bool : \"\"\"Helper function.\"\"\" return bool ( os . getenv ( environ_name , default ) . upper () in [ \"1\" , \"Y\" , \"YES\" , \"TRUE\" ])","title":"boolean_env()"},{"location":"api/#toshi_hazard_store.gridded_hazard_query","text":"Queries for saving and retrieving gridded hazard convenience.","title":"gridded_hazard_query"},{"location":"api/#toshi_hazard_store.gridded_hazard_query.get_gridded_hazard","text":"Fetch GriddedHazard based on criteria. Source code in toshi_hazard_store/gridded_hazard_query.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_gridded_hazard ( hazard_model_ids : Iterable [ str ], location_grid_ids : Iterable [ str ], vs30s : Iterable [ float ], imts : Iterable [ str ], aggs : Iterable [ str ], poes : Iterable [ float ], ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on criteria.\"\"\" # partition_key = f\"{obj.hazard_model_id}\" # sort_key = f\"{obj.hazard_model_id}:{obj.location_grid_id}:{obj.vs30}:{obj.imt}:{obj.agg}:{obj.poe}\" def build_sort_key ( hazard_model_id , grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build sort_key.\"\"\" sort_key = hazard_model_id sort_key = sort_key + f \": { sorted ( grid_ids )[ 0 ] } \" if grid_ids else sort_key sort_key = sort_key + f \": { sorted ( vs30s )[ 0 ] } \" if grid_ids and vs30s else sort_key sort_key = sort_key + f \": { sorted ( imts )[ 0 ] } \" if grid_ids and vs30s and imts else sort_key sort_key = sort_key + f \": { sorted ( aggs )[ 0 ] } \" if grid_ids and vs30s and imts and aggs else sort_key sort_key = sort_key + f \": { sorted ( poes )[ 0 ] } \" if grid_ids and vs30s and imts and aggs and poes else sort_key return sort_key def build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ): \"\"\"Build filter condition.\"\"\" condition_expr = mGH . hazard_model_id == hazard_model_id if location_grid_ids : condition_expr = condition_expr & mGH . location_grid_id . is_in ( * location_grid_ids ) if vs30s : condition_expr = condition_expr & mGH . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mGH . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mGH . agg . is_in ( * aggs ) if poes : condition_expr = condition_expr & mGH . poe . is_in ( * poes ) return condition_expr # TODO: this can be parallelised/optimised. for hazard_model_id in hazard_model_ids : sort_key_first_val = build_sort_key ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) condition_expr = build_condition_expr ( hazard_model_id , location_grid_ids , vs30s , imts , aggs , poes ) log . debug ( f 'sort_key_first_val { sort_key_first_val } ' ) log . debug ( f 'condition_expr { condition_expr } ' ) if sort_key_first_val : qry = mGH . query ( hazard_model_id , mGH . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mGH . query ( hazard_model_id , mGH . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_gridded_hazard()"},{"location":"api/#toshi_hazard_store.gridded_hazard_query.get_one_gridded_hazard","text":"Fetch GriddedHazard based on single criteria. Source code in toshi_hazard_store/gridded_hazard_query.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_one_gridded_hazard ( hazard_model_id : str , location_grid_id : str , vs30 : float , imt : str , agg : str , poe : float , ) -> Iterator [ mGH ]: \"\"\"Fetch GriddedHazard based on single criteria.\"\"\" qry = mGH . query ( hazard_model_id , mGH . sort_key == f ' { hazard_model_id } : { location_grid_id } : { vs30 } : { imt } : { agg } : { poe } ' ) log . debug ( f \"get_gridded_hazard: qry { qry } \" ) for hit in qry : yield ( hit )","title":"get_one_gridded_hazard()"},{"location":"api/#toshi_hazard_store.model","text":"","title":"model"},{"location":"api/#toshi_hazard_store.model.drop_tables","text":"Drop em Source code in toshi_hazard_store/model/__init__.py 19 20 21 22 def drop_tables (): \"\"\"Drop em\"\"\" drop_tables_v3 () drop_gridded ()","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/__init__.py 13 14 15 16 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" migrate_v3 () migrate_gridded ()","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.gridded_hazard","text":"This module defines the pynamodb tables used to store THH.","title":"gridded_hazard"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.CompressedJsonicAttribute","text":"Bases: Attribute A compressed, json serialisable model attribute Source code in toshi_hazard_store/model/gridded_hazard.py 24 25 26 27 28 29 30 31 32 33 34 35 class CompressedJsonicAttribute ( Attribute ): \"\"\" A compressed, json serialisable model attribute \"\"\" attr_type = STRING def serialize ( self , value : Any ) -> str : return compress_string ( json . dumps ( value )) # could this be pickle?? def deserialize ( self , value : str ) -> Union [ Dict , List ]: return json . loads ( decompress_string ( value ))","title":"CompressedJsonicAttribute"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.CompressedListAttribute","text":"Bases: CompressedJsonicAttribute A compressed list of floats attribute. Source code in toshi_hazard_store/model/gridded_hazard.py 38 39 40 41 42 43 44 45 46 47 48 class CompressedListAttribute ( CompressedJsonicAttribute ): \"\"\" A compressed list of floats attribute. \"\"\" def serialize ( self , value : List [ float ]) -> str : if value is not None and not isinstance ( value , list ): raise TypeError ( f \"value has invalid type ' { type ( value ) } '; List[float])expected\" , ) return super () . serialize ( value )","title":"CompressedListAttribute"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.GriddedHazard","text":"Bases: Model Grid points defined in location_grid_id has a values in grid_poes. Source code in toshi_hazard_store/model/gridded_hazard.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class GriddedHazard ( Model ): \"\"\"Grid points defined in location_grid_id has a values in grid_poes.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) sort_key = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_model_id = UnicodeAttribute () location_grid_id = UnicodeAttribute () vs30 = FloatAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () poe = FloatAttribute () grid_poes = CompressedListAttribute () @staticmethod def new_model ( hazard_model_id , location_grid_id , vs30 , imt , agg , poe , grid_poes ) -> 'GriddedHazard' : obj = GriddedHazard ( hazard_model_id = hazard_model_id , location_grid_id = location_grid_id , vs30 = vs30 , imt = imt , agg = agg , poe = poe , grid_poes = grid_poes , ) obj . partition_key = f \" { obj . hazard_model_id } \" obj . sort_key = f \" { obj . hazard_model_id } : { obj . location_grid_id } : { obj . vs30 } : { obj . imt } : { obj . agg } : { obj . poe } \" return obj","title":"GriddedHazard"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.GriddedHazard.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/gridded_hazard.py 54 55 56 57 58 59 60 61 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_GriddedHazard- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/gridded_hazard.py 107 108 109 110 111 112 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.gridded_hazard.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/gridded_hazard.py 98 99 100 101 102 103 104 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model","text":"This module defines the pynamodb tables used to store openquake data.","title":"openquake_v1_model"},{"location":"api/#toshi_hazard_store.model.openquake_v1_model.LevelValuePairAttribute","text":"Bases: MapAttribute Store the IMT level and the POE value at the level. Source code in toshi_hazard_store/model/openquake_v1_model.py 6 7 8 9 10 class LevelValuePairAttribute ( MapAttribute ): \"\"\"Store the IMT level and the POE value at the level.\"\"\" lvl = NumberAttribute ( null = False ) val = NumberAttribute ( null = False )","title":"LevelValuePairAttribute"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model","text":"This module defines the pynamodb tables used to store openquake v2 data.","title":"openquake_v2_model"},{"location":"api/#toshi_hazard_store.model.openquake_v2_model.IMTValuesAttribute","text":"Bases: MapAttribute Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists. Source code in toshi_hazard_store/model/openquake_v2_model.py 6 7 8 9 10 11 class IMTValuesAttribute ( MapAttribute ): \"\"\"Store the IntensityMeasureType e.g.(PGA, SA(N)) and the levels and values lists.\"\"\" imt = UnicodeAttribute () lvls = ListAttribute ( of = NumberAttribute ) vals = ListAttribute ( of = NumberAttribute )","title":"IMTValuesAttribute"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model","text":"This module defines the pynamodb tables used to store openquake data. Third iteration","title":"openquake_v3_model"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation","text":"Bases: LocationIndexedModel Stores aggregate hazard curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class HazardAggregation ( LocationIndexedModel ): \"\"\"Stores aggregate hazard curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_model_id = UnicodeAttribute () imt = UnicodeAttribute () agg = UnicodeAttribute () values = ListAttribute ( of = LevelValuePairAttribute ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1","title":"HazardAggregation"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 133 134 135 136 137 138 139 140 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_HazardAggregation- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 148 149 150 151 152 153 154 155 156 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { self . imt } : { self . agg } : { self . hazard_model_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.HazardAggregation.to_csv","text":"Generate lists ready for csv module - including a header, followed by n rows. Source code in toshi_hazard_store/model/openquake_v3_model.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @staticmethod def to_csv ( models : Iterable [ 'HazardAggregation' ]) -> Iterator [ Sequence [ Union [ str , float ]]]: \"\"\"Generate lists ready for csv module - including a header, followed by n rows.\"\"\" n_models = 0 for model in models : # create the header row, removing unneeded attributes if n_models == 0 : model_attrs = list ( model . attribute_values . keys ()) for attr in [ 'hazard_model_id' , 'uniq_id' , 'created' , 'nloc_0' , 'nloc_001' , 'nloc_01' , 'nloc_1' , 'partition_key' , 'sort_key' , 'values' , ]: model_attrs . remove ( attr ) levels = [ f 'poe- { value . lvl } ' for value in model . values ] yield ( model_attrs + levels ) # the data yield [ getattr ( model , attr ) for attr in model_attrs ] + [ value . val for value in model . values ] n_models += 1","title":"to_csv()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.LocationIndexedModel","text":"Bases: Model Model base class. Source code in toshi_hazard_store/model/openquake_v3_model.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class LocationIndexedModel ( Model ): \"\"\"Model base class.\"\"\" partition_key = UnicodeAttribute ( hash_key = True ) # For this we will use a downsampled location to 1.0 degree sort_key = UnicodeAttribute ( range_key = True ) nloc_001 = UnicodeAttribute () # 0.001deg ~100m grid nloc_01 = UnicodeAttribute () # 0.01deg ~1km grid nloc_1 = UnicodeAttribute () # 0.1deg ~10km grid nloc_0 = UnicodeAttribute () # 1.0deg ~100km grid version = VersionAttribute () uniq_id = UnicodeAttribute () lat = FloatAttribute () # latitude decimal degrees lon = FloatAttribute () # longitude decimal degrees vs30 = FloatAttribute () created = TimestampAttribute ( default = datetime_now ) def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"LocationIndexedModel"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.LocationIndexedModel.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" self . nloc_001 = location . downsample ( 0.001 ) . code self . nloc_01 = location . downsample ( 0.01 ) . code self . nloc_1 = location . downsample ( 0.1 ) . code self . nloc_0 = location . downsample ( 1.0 ) . code # self.nloc_10 = location.downsample(10.0).code self . lat = location . lat self . lon = location . lon self . uniq_id = str ( uuid . uuid4 ()) return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization","text":"Bases: LocationIndexedModel Stores the individual hazard realisation curves. Source code in toshi_hazard_store/model/openquake_v3_model.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class OpenquakeRealization ( LocationIndexedModel ): \"\"\"Stores the individual hazard realisation curves.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover hazard_solution_id = UnicodeAttribute () source_tags = UnicodeSetAttribute () source_ids = UnicodeSetAttribute () rlz = IntegerAttribute () # index of the openquake realization values = ListAttribute ( of = IMTValuesAttribute ) # Secondary Index attributes index1 = vs30_nloc1_gt_rlz_index () index1_rk = UnicodeAttribute () def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"OpenquakeRealization"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 191 192 193 194 195 196 197 198 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_OpenquakeRealization- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.OpenquakeRealization.set_location","text":"Set internal fields, indices etc from the location. Source code in toshi_hazard_store/model/openquake_v3_model.py 211 212 213 214 215 216 217 218 219 220 221 222 def set_location ( self , location : CodedLocation ): \"\"\"Set internal fields, indices etc from the location.\"\"\" super () . set_location ( location ) # update the indices rlzs = str ( self . rlz ) . zfill ( 6 ) vs30s = str ( self . vs30 ) . zfill ( VS30_KEYLEN ) self . partition_key = self . nloc_1 self . sort_key = f ' { self . nloc_001 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' self . index1_rk = f ' { self . nloc_1 } : { vs30s } : { rlzs } : { self . hazard_solution_id } ' return self","title":"set_location()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.ToshiOpenquakeMeta","text":"Bases: Model Stores metadata from the job configuration and the oq HDF5. Source code in toshi_hazard_store/model/openquake_v3_model.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ToshiOpenquakeMeta ( Model ): \"\"\"Stores metadata from the job configuration and the oq HDF5.\"\"\" class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover partition_key = UnicodeAttribute ( hash_key = True ) # a static value as we actually don't want to partition our data hazsol_vs30_rk = UnicodeAttribute ( range_key = True ) version = VersionAttribute () created = TimestampAttribute ( default = datetime_now ) hazard_solution_id = UnicodeAttribute () general_task_id = UnicodeAttribute () vs30 = NumberAttribute () # vs30 value imts = UnicodeSetAttribute () # list of IMTs locations_id = UnicodeAttribute () # Location codes identifier (ENUM?) source_ids = UnicodeSetAttribute () source_tags = UnicodeSetAttribute () inv_time = NumberAttribute () # Invesigation time in years # extracted from the OQ HDF5 src_lt = JSONAttribute () # sources meta as DataFrame JSON gsim_lt = JSONAttribute () # gmpe meta as DataFrame JSON rlz_lt = JSONAttribute () # realization meta as DataFrame JSON","title":"ToshiOpenquakeMeta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.ToshiOpenquakeMeta.Meta","text":"DynamoDB Metadata. Source code in toshi_hazard_store/model/openquake_v3_model.py 38 39 40 41 42 43 44 45 class Meta : \"\"\"DynamoDB Metadata.\"\"\" billing_mode = 'PAY_PER_REQUEST' table_name = f \"THS_WIP_OpenquakeMeta- { DEPLOYMENT_STAGE } \" region = REGION if IS_OFFLINE : host = \"http://localhost:8000\" # pragma: no cover","title":"Meta"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.vs30_nloc001_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 82 83 84 85 86 87 88 89 90 91 92 class vs30_nloc001_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs30:nloc_001:gtid:rlz6) 0.001 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index2_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc001_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.vs30_nloc1_gt_rlz_index","text":"Bases: LocalSecondaryIndex Local secondary index with vs#) + 0.1 Degree search resolution Source code in toshi_hazard_store/model/openquake_v3_model.py 69 70 71 72 73 74 75 76 77 78 79 class vs30_nloc1_gt_rlz_index ( LocalSecondaryIndex ): \"\"\" Local secondary index with vs#) + 0.1 Degree search resolution \"\"\" class Meta : # All attributes are projected projection = AllProjection () partition_key = UnicodeAttribute ( hash_key = True ) # Same as the base table index1_rk = UnicodeAttribute ( range_key = True )","title":"vs30_nloc1_gt_rlz_index"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.drop_tables","text":"Drop the tables, if they exist. Source code in toshi_hazard_store/model/openquake_v3_model.py 237 238 239 240 241 242 def drop_tables (): \"\"\"Drop the tables, if they exist.\"\"\" for table in tables : if table . exists (): # pragma: no cover table . delete_table () log . info ( f 'deleted table: { table } ' )","title":"drop_tables()"},{"location":"api/#toshi_hazard_store.model.openquake_v3_model.migrate","text":"Create the tables, unless they exist already. Source code in toshi_hazard_store/model/openquake_v3_model.py 228 229 230 231 232 233 234 def migrate (): \"\"\"Create the tables, unless they exist already.\"\"\" for table in tables : if not table . exists (): # pragma: no cover table . create_table ( wait = True ) print ( f \"Migrate created table: { table } \" ) log . info ( f \"Migrate created table: { table } \" )","title":"migrate()"},{"location":"api/#toshi_hazard_store.multi_batch","text":"","title":"multi_batch"},{"location":"api/#toshi_hazard_store.multi_batch.DynamoBatchWorker","text":"Bases: multiprocessing . Process A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. Source code in toshi_hazard_store/multi_batch.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DynamoBatchWorker ( multiprocessing . Process ): \"\"\"A worker that batches and saves records to DynamoDB. based on https://pymotw.com/2/multiprocessing/communication.html example 2. \"\"\" def __init__ ( self , task_queue , toshi_id , model ): multiprocessing . Process . __init__ ( self ) self . task_queue = task_queue # self.result_queue = result_queue self . toshi_id = toshi_id self . model = model self . batch_size = random . randint ( 15 , 50 ) def run ( self ): print ( f \"worker { self . name } running with batch size: { self . batch_size } \" ) proc_name = self . name models = [] while True : next_task = self . task_queue . get () if next_task is None : # Poison pill means shutdown print ( ' %s : Exiting' % proc_name ) # finally if len ( models ): self . _batch_save ( models ) self . task_queue . task_done () break assert isinstance ( next_task , self . model ) models . append ( next_task ) if len ( models ) > self . batch_size : self . _batch_save ( models ) models = [] self . task_queue . task_done () # self.result_queue.put(answer) return def _batch_save ( self , models ): # print(f\"worker {self.name} saving batch of len: {len(models)}\") if self . model == model . ToshiOpenquakeHazardCurveStatsV2 : query . batch_save_hcurve_stats_v2 ( self . toshi_id , models = models ) elif self . model == model . ToshiOpenquakeHazardCurveRlzsV2 : query . batch_save_hcurve_rlzs_v2 ( self . toshi_id , models = models ) elif self . model == model . OpenquakeRealization : with model . OpenquakeRealization . batch_write () as batch : for item in models : batch . save ( item ) else : raise ValueError ( \"WHATT!\" )","title":"DynamoBatchWorker"},{"location":"api/#toshi_hazard_store.oq_import","text":"","title":"oq_import"},{"location":"api/#toshi_hazard_store.oq_import.export_v3","text":"","title":"export_v3"},{"location":"api/#toshi_hazard_store.oq_import.export_v3.export_meta_v3","text":"Extract and same the meta data. Source code in toshi_hazard_store/oq_import/export_v3.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def export_meta_v3 ( dstore , toshi_hazard_id , toshi_gt_id , locations_id , source_tags , source_ids ): \"\"\"Extract and same the meta data.\"\"\" oq = dstore [ 'oqparam' ] source_lt , gsim_lt , rlz_lt = parse_logic_tree_branches ( dstore . filename ) df_len = 0 df_len += len ( source_lt . to_json ()) df_len += len ( gsim_lt . to_json ()) df_len += len ( rlz_lt . to_json ()) if df_len >= 300e3 : print ( 'WARNING: Dataframes for this job may be too large to store on DynamoDB.' ) obj = model . ToshiOpenquakeMeta ( partition_key = \"ToshiOpenquakeMeta\" , hazard_solution_id = toshi_hazard_id , general_task_id = toshi_gt_id , hazsol_vs30_rk = f \" { toshi_hazard_id } : { str ( int ( oq . reference_vs30_value )) . zfill ( 3 ) } \" , # updated=dt.datetime.now(tzutc()), # known at configuration vs30 = int ( oq . reference_vs30_value ), # vs30 value imts = list ( oq . imtls . keys ()), # list of IMTs locations_id = locations_id , # Location code or list ID source_tags = source_tags , source_ids = source_ids , inv_time = vars ( oq )[ 'investigation_time' ], src_lt = source_lt . to_json (), # sources meta as DataFrame JSON gsim_lt = gsim_lt . to_json (), # gmpe meta as DataFrame JSON rlz_lt = rlz_lt . to_json (), # realization meta as DataFrame JSON ) obj . save () return OpenquakeMeta ( source_lt , gsim_lt , rlz_lt , obj )","title":"export_meta_v3()"},{"location":"api/#toshi_hazard_store.pynamodb_settings","text":"pynamodb_settings. Default settings may be overridden by providing a Python module which exports the desired new values. Set the PYNAMODB_CONFIG environment variable to an absolute path to this module or write it to /etc/pynamodb/ global_default_settings.py to have it automatically discovered.","title":"pynamodb_settings"},{"location":"api/#toshi_hazard_store.query_v3","text":"Queries for saving and retrieving openquake hazard results with convenience.","title":"query_v3"},{"location":"api/#toshi_hazard_store.query_v3.first_vs30_key","text":"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. Source code in toshi_hazard_store/query_v3.py 61 62 63 64 65 66 67 68 69 70 71 def first_vs30_key ( vs30s ): \"\"\"This function handles vs30 index keys with variable length (3 or 4), which occur since the addition of vs30 values 1000 & 1500. DynamoDB sort key is not mutable, so we must handle this in our query instead, which is slighlty less efficient. Leave this in place unldess the table can be rewritten with a new vs30 key length of 4 characters. \"\"\" if have_mixed_length_vs30s ( vs30s ): vsLong = filter ( lambda x : x >= 1000 , vs30s ) return str ( int ( min ( vsLong ) / 10 )) . zfill ( model . VS30_KEYLEN ) return str ( min ( vs30s )) . zfill ( model . VS30_KEYLEN )","title":"first_vs30_key()"},{"location":"api/#toshi_hazard_store.query_v3.get_hazard_curves","text":"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' Source code in toshi_hazard_store/query_v3.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def get_hazard_curves ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s hazard_model_ids : Iterable [ str ] = [], # hazard_model_ids imts : Iterable [ str ] = [], aggs : Iterable [ str ] = [], ) -> Iterator [ mHAG ]: \"\"\"Use mHAG.sort_key as much as possible. f'{nloc_001}:{vs30s}:{hazard_model_id}' \"\"\" def build_condition_expr ( locs , vs30s , hids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mHAG . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mHAG . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mHAG . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mHAG . vs30 . is_in ( * vs30s ) if imts : condition_expr = condition_expr & mHAG . imt . is_in ( * imts ) if aggs : condition_expr = condition_expr & mHAG . agg . is_in ( * aggs ) if hids : condition_expr = condition_expr & mHAG . hazard_model_id . is_in ( * hids ) return condition_expr def build_sort_key ( locs , vs30s , hids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and imts : first_imt = sorted ( imts )[ 0 ] sort_key_first_val += f \": { first_imt } \" if vs30s and imts and aggs : first_agg = sorted ( aggs )[ 0 ] sort_key_first_val += f \": { first_agg } \" if vs30s and imts and aggs and hids : first_hid = sorted ( hids )[ 0 ] sort_key_first_val += f \": { first_hid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): log . debug ( 'hash_key %s ' % hash_location_code ) hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , hazard_model_ids ) condition_expr = build_condition_expr ( hash_locs , vs30s , hazard_model_ids ) log . debug ( 'sort_key_first_val: %s ' % sort_key_first_val ) log . debug ( 'condition_expr: %s ' % condition_expr ) if sort_key_first_val : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mHAG . query ( hash_location_code , mHAG . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) log . debug ( \"get_hazard_rlz_curves_v3: qry %s \" % qry ) for hit in qry : yield ( hit )","title":"get_hazard_curves()"},{"location":"api/#toshi_hazard_store.query_v3.get_hazard_metadata_v3","text":"Fetch ToshiOpenquakeHazardMeta based on criteria. Source code in toshi_hazard_store/query_v3.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_hazard_metadata_v3 ( haz_sol_ids : Iterable [ str ] = None , vs30_vals : Iterable [ int ] = None , ) -> Iterator [ mOQM ]: \"\"\"Fetch ToshiOpenquakeHazardMeta based on criteria.\"\"\" condition_expr = None if haz_sol_ids : condition_expr = condition_expr & mOQM . hazard_solution_id . is_in ( * haz_sol_ids ) if vs30_vals : condition_expr = condition_expr & mOQM . vs30 . is_in ( * vs30_vals ) for hit in mOQM . query ( \"ToshiOpenquakeMeta\" , filter_condition = condition_expr # NB the partition key is the table name! ): yield ( hit )","title":"get_hazard_metadata_v3()"},{"location":"api/#toshi_hazard_store.query_v3.get_rlz_curves_v3","text":"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' Source code in toshi_hazard_store/query_v3.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_rlz_curves_v3 ( locs : Iterable [ str ] = [], # nloc_001 vs30s : Iterable [ int ] = [], # vs30s rlzs : Iterable [ int ] = [], # rlzs tids : Iterable [ str ] = [], # toshi hazard_solution_ids imts : Iterable [ str ] = [], ) -> Iterator [ mRLZ ]: \"\"\"Use mRLZ.sort_key as much as possible. f'{nloc_001}:{vs30s}:{rlzs}:{self.hazard_solution_id}' \"\"\" def build_condition_expr ( locs , vs30s , rlzs , tids ): \"\"\"Build filter condition.\"\"\" ## TODO REFACTOR ME ... using the res of first loc is not ideal grid_res = decimal . Decimal ( str ( list ( locs )[ 0 ] . split ( '~' )[ 0 ])) places = grid_res . as_tuple () . exponent # print() # print(f'places {places} loc[0] {locs[0]}') res = float ( decimal . Decimal ( 10 ) ** places ) locs = [ downsample_code ( loc , res ) for loc in locs ] condition_expr = None if places == - 1 : condition_expr = condition_expr & mRLZ . nloc_1 . is_in ( * locs ) if places == - 2 : condition_expr = condition_expr & mRLZ . nloc_01 . is_in ( * locs ) if places == - 3 : condition_expr = condition_expr & mRLZ . nloc_001 . is_in ( * locs ) if vs30s : condition_expr = condition_expr & mRLZ . vs30 . is_in ( * vs30s ) if rlzs : condition_expr = condition_expr & mRLZ . rlz . is_in ( * rlzs ) if tids : condition_expr = condition_expr & mRLZ . hazard_solution_id . is_in ( * tids ) return condition_expr def build_sort_key ( locs , vs30s , rlzs , tids ): \"\"\"Build sort_key.\"\"\" sort_key_first_val = \"\" first_loc = sorted ( locs )[ 0 ] # these need to be formatted to match the sort key 0.001 ? sort_key_first_val += f \" { first_loc } \" if vs30s : sort_key_first_val += f \": { first_vs30_key ( vs30s ) } \" if have_mixed_length_vs30s ( vs30s ): # we must stop the sort_key build here return sort_key_first_val if vs30s and rlzs : first_rlz = str ( sorted ( rlzs )[ 0 ]) . zfill ( 6 ) sort_key_first_val += f \": { first_rlz } \" if vs30s and rlzs and tids : first_tid = sorted ( tids )[ 0 ] sort_key_first_val += f \": { first_tid } \" return sort_key_first_val # print('hashes', get_hashes(locs)) # TODO: use https://pypi.org/project/InPynamoDB/ for hash_location_code in get_hashes ( locs ): # print(f'hash_key {hash_location_code}') hash_locs = list ( filter ( lambda loc : downsample_code ( loc , 0.1 ) == hash_location_code , locs )) sort_key_first_val = build_sort_key ( hash_locs , vs30s , rlzs , tids ) condition_expr = build_condition_expr ( hash_locs , vs30s , rlzs , tids ) # print(f'sort_key_first_val: {sort_key_first_val}') # print(f'condition_expr: {condition_expr}') # expected_sort_key = '-41.300~174.780:750:000000:A_CRU' # expected_hash_key = '-41.3~174.8' # print() # print(expected_hash_key, expected_sort_key) # # assert 0 if sort_key_first_val : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= sort_key_first_val , filter_condition = condition_expr ) else : qry = mRLZ . query ( hash_location_code , mRLZ . sort_key >= \" \" , # lowest printable char in ascii table is SPACE. (NULL is first control) filter_condition = condition_expr , ) # print(f\"get_hazard_rlz_curves_v3: qry {qry}\") for hit in qry : if imts : hit . values = list ( filter ( lambda x : x . imt in imts , hit . values )) yield ( hit )","title":"get_rlz_curves_v3()"},{"location":"api/#toshi_hazard_store.query_v3.have_mixed_length_vs30s","text":"Does the list of vs30s require mixed length index keys. Source code in toshi_hazard_store/query_v3.py 50 51 52 53 54 55 56 57 58 def have_mixed_length_vs30s ( vs30s ): \"\"\"Does the list of vs30s require mixed length index keys.\"\"\" max_vs30 = max ( vs30s ) min_vs30 = min ( vs30s ) if max_vs30 >= 1000 : if min_vs30 < 1000 : return True else : return False","title":"have_mixed_length_vs30s()"},{"location":"api/#toshi_hazard_store.transform","text":"Helper functions to export an openquake calculation and save it with toshi-hazard-store.","title":"transform"},{"location":"api/#toshi_hazard_store.transform.parse_logic_tree_branches","text":"Extract the dataframes. Source code in toshi_hazard_store/transform.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def parse_logic_tree_branches ( file_id ): \"\"\"Extract the dataframes.\"\"\" with h5py . File ( file_id ) as hf : # read and prepare the source model logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of source_model_lt into a dataframe source_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'source_model_lt' ][:]) for col in source_lt . columns [: - 1 ]: source_lt . loc [:, col ] = source_lt [ col ] . str . decode ( 'ascii' ) # identify the source labels used in the realizations table source_lt . loc [:, 'branch_code' ] = [ x for x in BASE183 [ 0 : len ( source_lt )]] source_lt . set_index ( 'branch_code' , inplace = True ) # read and prepare the gsim logic tree for documentation ### full_lt is a key that contains subkeys for each type of logic tree ### here we read the contents of gsim_lt into a dataframe gsim_lt = pd . DataFrame ( hf [ 'full_lt' ][ 'gsim_lt' ][:]) for col in gsim_lt . columns [: - 1 ]: gsim_lt . loc [:, col ] = gsim_lt . loc [:, col ] . str . decode ( 'ascii' ) # break up the gsim df into tectonic regions (one df per column of gsims in realization labels. e.g. A~AAA) # the order of the dictionary is consistent with the order of the columns gsim_lt_dict = {} for i , trt in enumerate ( np . unique ( gsim_lt [ 'trt' ])): df = gsim_lt [ gsim_lt [ 'trt' ] == trt ] df . loc [:, 'branch_code' ] = [ x [ 1 ] for x in df [ 'branch' ]] df . set_index ( 'branch_code' , inplace = True ) ### the branch code used to be a user specified string from the gsim logic tree .xml ### now the only way to identify which regionalization is used is to extract it manually for j , x in zip ( df . index , df [ 'uncertainty' ]): tags = re . split ( ' \\\\ [| \\\\ ]| \\n region = \\\" | \\\" ' , x ) if len ( tags ) > 4 : df . loc [ j , 'model name' ] = f ' { tags [ 1 ] } _ { tags [ 3 ] } ' else : df . loc [ j , 'model name' ] = tags [ 1 ] gsim_lt_dict [ i ] = df # read and prep the realization record for documentation ### this one can be read into a df directly from the dstore's full_lt ### the column titled 'ordinal' is dropped, as it will be the same as the 0-n index dstore = datastore . read ( file_id ) rlz_lt = pd . DataFrame ( dstore [ 'full_lt' ] . rlzs ) . drop ( 'ordinal' , axis = 1 ) # add to the rlt_lt to note which source models and which gsims were used for each branch for i_rlz in rlz_lt . index : # rlz name is in the form A~AAA, with a single source identifier followed by characters for each trt region srm_code , gsim_codes = rlz_lt . loc [ i_rlz , 'branch_path' ] . split ( '~' ) # copy over the source label rlz_lt . loc [ i_rlz , 'source combination' ] = source_lt . loc [ srm_code , 'branch' ] # loop through the characters for the trt region and add the corresponding gsim name for i , gsim_code in enumerate ( gsim_codes ): trt , gsim = gsim_lt_dict [ i ] . loc [ gsim_code , [ 'trt' , 'model name' ]] rlz_lt . loc [ i_rlz , trt ] = gsim return source_lt , gsim_lt , rlz_lt","title":"parse_logic_tree_branches()"},{"location":"api/#toshi_hazard_store.utils","text":"Common utilities.","title":"utils"},{"location":"api/#toshi_hazard_store.utils.normalise_site_code","text":"Return a valid code for storage. Source code in toshi_hazard_store/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def normalise_site_code ( oq_site_object : tuple , force_normalized : bool = False ) -> CodedLocation : \"\"\"Return a valid code for storage.\"\"\" if len ( oq_site_object ) not in [ 2 , 3 ]: raise ValueError ( f \"Unknown site object { oq_site_object } \" ) force_normalized = force_normalized if len ( oq_site_object ) == 3 else True if len ( oq_site_object ) == 3 : _ , lon , lat = oq_site_object elif len ( oq_site_object ) == 2 : lon , lat = oq_site_object rounded = CodedLocation ( lon = lon , lat = lat , resolution = 0.001 ) if not force_normalized : rounded . code = oq_site_object [ 0 ] . decode () # restore the original location code return rounded","title":"normalise_site_code()"},{"location":"changelog/","text":"Changelog \u00b6 [0.5.5] - 2022-10-06 \u00b6 Changed \u00b6 fix for queries with mixed length vs30 index keys migrate more print statements to logging.debug [0.5.4] - 2022-09-27 \u00b6 Added \u00b6 new query get_one_gridded_hazard -m option to script to export meta tables only Changed \u00b6 migrated print statements to logging.debug removed monkey patch for BASE183 - it iss in oqengine 3.15 now more test cover [0.5.3] - 2022-08-18 \u00b6 Changed \u00b6 using nzshm-common==0.3.2 from pypi. specify poetry==1.2.0b3 in all the GHA yml files. [0.5.1] - 2022-08-17 \u00b6 Added \u00b6 THS_HazardAggregation table support for csv serialisation. Changed \u00b6 refactoring/renaming openquake import modules. Removed \u00b6 one openquake test no longer works as expected. It's off-piste so skipping it for now. data_functions migrated to THP branch_combinator migrated to THP [0.5.0] - 2022-08-03 \u00b6 Added \u00b6 V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling. Removed \u00b6 realisation aggregration computations. These have moving to toshi-hazard-post [0.4.1] - 2022-06-22 \u00b6 Added \u00b6 multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types Changed \u00b6 V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules. [0.4.0] - 2022-06-10 \u00b6 Added \u00b6 new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency. Changed \u00b6 meta dataframes are cut back to dstore defaults to minimise size. [0.3.2] - 2022-05-30 \u00b6 Added \u00b6 meta.aggs attribute meta.inv_tme attribute Changed \u00b6 store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks). [0.3.1] - 2022-05-29 \u00b6 Changed \u00b6 updated usage. [0.3.0] - 2022-05-28 \u00b6 Added \u00b6 store_hazard script for openquake systems. Changed \u00b6 tightened up model attributes names. [0.2.0] - 2022-05-27 \u00b6 Added \u00b6 query api improvements added meta table new query methods for meta and rlzs Changed \u00b6 moved vs30 from curves to meta updated docs [0.1.3] - 2022-05-26 \u00b6 Changed \u00b6 fixed mkdoc rendering of python & markdown. [0.1.2] - 2022-05-26 \u00b6 Changed \u00b6 fix poetry lockfile [0.1.1] - 2022-05-26 \u00b6 Added \u00b6 First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#055---2022-10-06","text":"","title":"[0.5.5] - 2022-10-06"},{"location":"changelog/#changed","text":"fix for queries with mixed length vs30 index keys migrate more print statements to logging.debug","title":"Changed"},{"location":"changelog/#054---2022-09-27","text":"","title":"[0.5.4] - 2022-09-27"},{"location":"changelog/#added","text":"new query get_one_gridded_hazard -m option to script to export meta tables only","title":"Added"},{"location":"changelog/#changed_1","text":"migrated print statements to logging.debug removed monkey patch for BASE183 - it iss in oqengine 3.15 now more test cover","title":"Changed"},{"location":"changelog/#053---2022-08-18","text":"","title":"[0.5.3] - 2022-08-18"},{"location":"changelog/#changed_2","text":"using nzshm-common==0.3.2 from pypi. specify poetry==1.2.0b3 in all the GHA yml files.","title":"Changed"},{"location":"changelog/#051---2022-08-17","text":"","title":"[0.5.1] - 2022-08-17"},{"location":"changelog/#added_1","text":"THS_HazardAggregation table support for csv serialisation.","title":"Added"},{"location":"changelog/#changed_3","text":"refactoring/renaming openquake import modules.","title":"Changed"},{"location":"changelog/#removed","text":"one openquake test no longer works as expected. It's off-piste so skipping it for now. data_functions migrated to THP branch_combinator migrated to THP","title":"Removed"},{"location":"changelog/#050---2022-08-03","text":"","title":"[0.5.0] - 2022-08-03"},{"location":"changelog/#added_2","text":"V3 THS table models with improved indexing and and performance (esp. THS_HazardAggregation table) using latest CodedLocation API to manage gridded lcoations and resampling.","title":"Added"},{"location":"changelog/#removed_1","text":"realisation aggregration computations. These have moving to toshi-hazard-post","title":"Removed"},{"location":"changelog/#041---2022-06-22","text":"","title":"[0.4.1] - 2022-06-22"},{"location":"changelog/#added_3","text":"multi_batch module for parallelised batch saves DESIGN.md capture notes on the experiments, test and mods to the package new switch on V2 queries to force normalised_location_id new '-f' switch on store_hazard script to force normalised_location_id lat, lon Float fields to support numeric range filtering in queries created timestamp field on stas, rlzs v2 added pynamodb_attributes for FloatAttribute, TimestampAttribute types","title":"Added"},{"location":"changelog/#changed_4","text":"V2 store queries will automatically use nomralised location if custom sites aren't available. refactored model modules.","title":"Changed"},{"location":"changelog/#040---2022-06-10","text":"","title":"[0.4.0] - 2022-06-10"},{"location":"changelog/#added_4","text":"new V2 models for stats and rlzs. new get_hazard script for manual testing. extra test coverage with optional openquake install as DEV dependency.","title":"Added"},{"location":"changelog/#changed_5","text":"meta dataframes are cut back to dstore defaults to minimise size.","title":"Changed"},{"location":"changelog/#032---2022-05-30","text":"","title":"[0.3.2] - 2022-05-30"},{"location":"changelog/#added_5","text":"meta.aggs attribute meta.inv_tme attribute","title":"Added"},{"location":"changelog/#changed_6","text":"store hazard can create tables. store hazard adds extra meta. store hazard truncates values for rlz and agg fields. make stats & rlz queries tolerant to ID-only form (fails with REAL dynamodb & not in mocks).","title":"Changed"},{"location":"changelog/#031---2022-05-29","text":"","title":"[0.3.1] - 2022-05-29"},{"location":"changelog/#changed_7","text":"updated usage.","title":"Changed"},{"location":"changelog/#030---2022-05-28","text":"","title":"[0.3.0] - 2022-05-28"},{"location":"changelog/#added_6","text":"store_hazard script for openquake systems.","title":"Added"},{"location":"changelog/#changed_8","text":"tightened up model attributes names.","title":"Changed"},{"location":"changelog/#020---2022-05-27","text":"","title":"[0.2.0] - 2022-05-27"},{"location":"changelog/#added_7","text":"query api improvements added meta table new query methods for meta and rlzs","title":"Added"},{"location":"changelog/#changed_9","text":"moved vs30 from curves to meta updated docs","title":"Changed"},{"location":"changelog/#013---2022-05-26","text":"","title":"[0.1.3] - 2022-05-26"},{"location":"changelog/#changed_10","text":"fixed mkdoc rendering of python & markdown.","title":"Changed"},{"location":"changelog/#012---2022-05-26","text":"","title":"[0.1.2] - 2022-05-26"},{"location":"changelog/#changed_11","text":"fix poetry lockfile","title":"Changed"},{"location":"changelog/#011---2022-05-26","text":"","title":"[0.1.1] - 2022-05-26"},{"location":"changelog/#added_8","text":"First release on PyPI. query and model modules providing basic support for openquake hazard stats curves only.","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"toshi-hazard-store could always use more documentation, whether as part of the official toshi-hazard-store docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/GNS-Science/toshi-hazard-store/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up toshi-hazard-store for local development. Fork the toshi-hazard-store repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/toshi-hazard-store.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/GNS-Science/toshi-hazard-store/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_toshi_hazard_store.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install toshi-hazard-store, run this command in your terminal: $ pip install toshi-hazard-store This is the preferred method to install toshi-hazard-store, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for toshi-hazard-store can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/GNS-Science/toshi-hazard-store Or download the tarball : $ curl -OJL https://github.com/GNS-Science/toshi-hazard-store/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 Environment & Authorisation pre-requisites \u00b6 NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication) toshi-hazard-store (library) \u00b6 To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break store_hazard (script) \u00b6 TODO decribe usage of the upload script","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#environment--authorisation-pre-requisites","text":"NZSHM22_HAZARD_STORE_STAGE=XXXX (TEST or PROD) NZSHM22_HAZARD_STORE_REGION=XXXXX (ap-southeast-2) AWS_PROFILE- ... (See AWS authentication)","title":"Environment &amp; Authorisation pre-requisites"},{"location":"usage/#toshi-hazard-store-library","text":"To use toshi-hazard-store in a project from toshi_hazard_store import query import pandas as pd import json TOSHI_ID = \"abcdef\" ## get some solution meta data ... for m in query.get_hazard_metadata(None, vs30_vals=[250, 350]): print(m.vs30, m.haz_sol_id, m.locs) source_lt = pd.read_json(m.src_lt) gsim_lt = pd.read_json(m.gsim_lt) rlzs_df = pd.read_json(m.rlz_lt) # realizations meta as pandas datframe. rlzs_dict = json.loads(m.rlz_lt) # realizations meta as dict. print(rlzs_dict) print(rlzs_df) ## get some agreggate curves for r in query.get_hazard_stats_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD'], ['mean']): print(\"stat\", r.loc, r.values[0]) break ## get some realisation curves for r in query.get_hazard_rlz_curves(m.haz_sol_id, ['PGA'], ['WLG', 'QZN', 'CHC', 'DUD']): print(\"rlz\", r.loc, r.rlz, r.values[0] ) break","title":"toshi-hazard-store (library)"},{"location":"usage/#store_hazard-script","text":"TODO decribe usage of the upload script","title":"store_hazard (script)"}]}